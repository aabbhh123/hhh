##### PRACTICAL NO.-1 ######

##### Practical 1-A ######

Installation.

##### Practical 1-B ######

##### Aim: Convert the given text to speech. #####

pip install gTTS
from gtts import gTTS  

pip install playsound
from playsound import playsound  

text_val = 'All the best for your exam.'

language = 'en'

obj = gTTS(text=text_val, lang=language, slow=False)

obj.save("exam.mp3")  

playsound("exam.mp3")


##### Practical 1-C ######

##### Aim: Convert the Speech of .wav audio file to Text. #####

pip install SpeechRecognition pydub

#import library
import speech_recognition as sr

# initialize the recognizer
# open the file

r= sr.Recognizer()

with sr.AudioFile("male.wav") as source:
    # listen for the data (load audio to memory)
    audio_data = r.record(source)
    # recognize (convert from speech to text)
    text = r.recognize_google(audio_data)
    print(text)






##### PRACTICAL NO.-2 ######

##### Practical 2-A ######

##### Aim: Study of various Corpus – Brown, Inaugural, Reuters, udhr with various methods like filelds, raw, words, sents, categories. #####

import nltk
from nltk.corpus import brown
nltk.download('brown')
print('File ids of brown corpus\n',brown.fileids())
ca01 = brown.words('ca01')
#display first few words
print('\nca01 has following words:\n',ca01)
#total jumber of words in ca01
print('\nca01 has',len(ca01),'words')

#categories or files 
print('\n\nCategories or file in brown corpus:\n') 
print(brown.categories()) 

print('\n\nStatistics for each text:\n') 
print('AvgWordLen\tAvgSentenceLen\tno.ofTimesEachWordApperarsOnAvg\t\tFileName') 
for fileid in brown.fileids(): 
    num_chars = len(brown.raw(fileid)) 
    num_words = len(brown.words(fileid)) 
    num_sents = len(brown.sents(fileid)) 
    num_vocab = len(set([w.lower() for w in brown.words(fileid)])) 
    print(int(num_chars/num_words),'\t\t\t',int(num_words/num_sents),'\t\t\t',int(num_words/num_vocab),'\t\t\t',fileid)


##### Practical 2-B ######

##### Aim: Create and use your own corpora (plaintext, categorical) #####

import nltk
from nltk.corpus import PlaintextCorpusReader
corpus_root='F:\Corpus'
filelist=PlaintextCorpusReader(corpus_root,'.*')
print('\n File list: \n')
print(filelist.fileids())

print(filelist.root)

import nltk
nltk.download('punkt')
print('\n\n Statistics for each text:\n')
print('AvgWordLen\tAvgSentenceLen\tno.ofTimesEachWordAppearsOnAvg\tFileName')
for fileid in filelist.fileids():
    num_chars=len(filelist.raw(fileid))
    num_words=len(filelist.words(fileid))
    num_sents=len(filelist.sents(fileid))
    num_vocab=len(set([w.lower() for w in filelist.words(fileid)]))
    print(int(num_chars/num_words),'\t\t\t',int(num_words/num_sents),'\t\t\t',int(num_words/num_vocab),'\t\t\t',fileid)


##### Practical 2-C & 2-D Combine ######

##### Aim: Study of tagged corpora with methods like tagged_sents, tagged_words. & Write a program to find the most frequent noun tags. #####

import nltk
from collections import defaultdict
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

text = nltk.word_tokenize("Chaitu likes to play football. Chaitu does not like to play cricket.")
tagged = nltk.pos_tag(text)
print(tagged)

# Checking if it is a noun or not.
addNounWords = []
count=0
for words in tagged:
  val = tagged[count][1]
  if(val == 'NN' or val == 'NNS' or val == 'NNPS' or val == 'NNP'):
    addNounWords.append(tagged[count][0])
  count+=1
print(addNounWords)

temp = defaultdict(int)
# memoizing count
for sub in addNounWords:
  temp[sub] += 1

# getting max frequency
res = max(temp, key=temp.get)

# printing result
print("Word with maximum frequency : " + str(res))






##### PRACTICAL NO.-3 ######

##### Practical 3-A ######

##### Aim: Study of Wordnet Dictionary with methods as synsets, definitions, examples, antonyms. #####

import nltk
nltk.download('omw-1.4')

import nltk
from nltk.corpus import wordnet
nltk.download('wordnet')
print(wordnet.synsets("computer"))

# defination and example of the word 'computer'
print(wordnet.synset("computer.n.01").definition())

# examples
print("Examples:",wordnet.synset("computer.n.01").examples())

# get Antonyms
print(wordnet.lemma('buy.v.01.buy').antonyms())


##### Practical 3-B ######

##### Aim: Study lemmas, hyponyms, hypernyms. #####

import nltk
from nltk.corpus import wordnet
nltk.download('wordnet')
print(wordnet.synsets("computer"))

print(wordnet.synset("computer.n.01").lemma_names())

# all lemmas for each synset.
for e in wordnet.synsets("computer"):
  print(f'{e} --> {e.lemma_names()}')

# print all lemmas for a given synset.
print(wordnet.synset('computer.n.01').lemmas())

# get the synset corresponding to lemma
print(wordnet.lemma('computer.n.01.computing_device').synset())

# Get the name of the lemma.
print(wordnet.lemma('computer.n.01.computing_device').name())

# Hyponyms give abstract concepts of the word that are much more specific.
# the list of hyponyms words of the computer
syn = wordnet.synset('computer.n.01')
print(syn.hyponyms)
print([lemma.name() for synset in syn.hyponyms() for lemma in synset.lemmas()])

# the semantic similarity in wordnet.
vehicle = wordnet.synset('vehicle.n.01')
car = wordnet.synset('car.n.01')
print(car.lowest_common_hypernyms(vehicle))



##### Practical 3-C ######

##### Aim: sentence tokenization, word tokenization, Part of speech Tagging and chunking of user defined text. #####

import nltk
from nltk import tokenize
nltk.download('punkt')
from nltk import tag
from nltk import chunk
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')

para = "Hello! My name is Chaitanya Mane. Today you'll be learning NLTK."
sents = tokenize.sent_tokenize(para)
print("\nsentence tokenization\n=================\n",sents)

# word tokenization 
print("\nword tokenization\n===================\n") 
for index in range(len(sents)): 
  words = tokenize.word_tokenize(sents[index]) 
  print(words)

# POS Tagging 
tagged_words = [] 
for index in range(len(sents)): 
  tagged_words.append(tag.pos_tag(words)) 
  print("\nPOS Tagging\n===========\n",tagged_words)

# chunking 
tree = [] 
for index in range(len(sents)): 
  tree.append(chunk.ne_chunk(tagged_words[index])) 
  print("\nchunking\n========\n") 
  print(tree)






##### PRACTICAL NO.-4 ######

##### Practical 4-A ######

##### Aim: Named Entity recognition using user defined text. #####

!pip install spacy

!python -m spacy download en_core_web_sm

import spacy

# Load English tokenizer, tagger, parser and NER
nlp = spacy.load("en_core_web_sm")

# Process whole documents
text = ("When Sebastian Thrun started working on self-driving cars at"
        "Google in 2007 , few people outside of the company took him"
        "seriously. I can tell you very senior CEOs of major American"
        "car companies would shake my hand and turn away because I wasn't"
        "worth talking to, said Thrun, in an interview with Recode earlier"
        "this week.")

doc = nlp(text)

# Analyse syntax
print("Noun phrases:",[chunk.text for chunk in doc.noun_chunks])
print("Verbs:", [token.lemma_ for token in doc if token.pos_ == "VERB"])


##### Practical 4-B ######

##### Aim: Named Entity recognition with diagram using NLTK corpus – treebank. #####

!pip install nltk

import nltk
nltk.download('treebank')

from nltk.corpus import treebank_chunk
treebank_chunk.tagged_sents()[0]

treebank_chunk.chunked_sents()[0]
treebank_chunk.chunked_sents()[0].draw()






##### PRACTICAL NO.-5 ######

##### Practical 5-A ######

##### Aim: Chart parsing using the string "Book that flight". #####

import nltk
from nltk import tokenize

grammer1 = nltk.CFG.fromstring("""
S -> VP
        VP -> VP NP
        NP -> Det NP
        Det -> 'that'
        NP -> singular Noun
        NP -> 'flight'
        VP -> 'Book'
""")
sentence = "Book that flight"

import nltk
nltk.download('punkt')

for index in range(len(sentence)):
    all_tokens = tokenize.word_tokenize(sentence)

print(all_tokens)
parser = nltk.ChartParser(grammer1)
for tree in parser.parse(all_tokens):
    print(tree)
    tree.draw()


##### Practical 5-B ######

##### Aim: Chart parsing using the string "I saw a bird in my balcony". #####

import nltk
from nltk import tokenize

grammer1 = nltk.CFG.fromstring("""
S -> NP VP
PP -> P NP
NP -> Det N | Det N PP | 'I'
VP -> V NP | VP PP
Det -> 'a' | 'my'
N -> 'bird' | 'balcony'
V -> 'saw'
P -> 'in'
""")
sentence = "I saw a bird in my balcony"

import nltk
nltk.download('punkt')

for index in range(len(sentence)):
    all_tokens = tokenize.word_tokenize(sentence)
print(all_tokens)

parser = nltk.ChartParser(grammer1)
for tree in parser.parse(all_tokens):
    print(tree)
    tree.draw()







##### PRACTICAL NO.-6 ######

##### Practical 6-A ######

##### Aim: Analysing the meaning of sentence by querying a database. #####

import nltk
from nltk import load_parser

nltk.download('book_grammars')
nltk.data.show_cfg('grammars/book_grammars/sql0.fcfg')

from nltk.parse import load_parser
cp=load_parser('grammars/book_grammars/sql0.fcfg')
query='What cities are located in China'
trees=list(cp.parse(query.split()))
print(trees)
answer=trees[0].label()['SEM']
print(answer)

answer=[s for s in answer if s]

q=' '.join(answer)
print(q)

from nltk.sem import chat80
nltk.download('city_database')

rows=chat80.sql_query('corpora/city_database/city.db',q)

for r in rows:
  print(r)





##### Practical 6-B ######

##### Aim: Building a Discourse Representation Theory (DRT) by parsing a string representation. #####

import nltk
read_the_expr=nltk.sem.DrtExpression.fromstring

drs1=read_the_expr('([x,y],[Angus(x),dog(y),own(x,y)])')
print(drs1)
drs1.draw()

print(drs1.fol())

from nltk import load_parser
parser = load_parser('grammars/book_grammars/drt.fcfg', logic_parser=nltk.sem.drt.DrtParser())
trees = list(parser.parse('Angus owns a dog'.split()))
print(trees[0].label()['SEM'].simplify())
trees[0].draw()






##### PRACTICAL NO.-7 ######

##### Practical 7 ######

##### Aim: Study PorterStemmer, LancasterStemmer, RegexpStemmer, SnowballStemmer and WordNetLemmatizer #####

import nltk
nltk.download('wordnet')

#PorterStemmer
import nltk
from nltk.stem import PorterStemmer
word_stemmer = PorterStemmer()
print(word_stemmer.stem('writing'))

#LancasterStemmer
import nltk
from nltk.stem import LancasterStemmer
Lanc_stemmer = LancasterStemmer()
print(Lanc_stemmer.stem('writing'))

#RegexpStemmer
import nltk
from nltk.stem import RegexpStemmer
Reg_stemmer = RegexpStemmer('ing$|s$|e$|able$', min=4)
print(Reg_stemmer.stem('writing'))

#SnowballStemmer
import nltk
from nltk.stem import SnowballStemmer
english_stemmer = SnowballStemmer('english')
print(english_stemmer.stem ('writing'))

#WordNetLemmatizer
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
print("word :\tlemma") 
print("rocks :", lemmatizer.lemmatize("rocks"))
print("corpora :", lemmatizer.lemmatize("corpora"))

#a denotes adjective in "pos"
print("better :", lemmatizer.lemmatize("better", pos ="a"))






##### PRACTICAL NO.-8 ######

##### Practical 8-A ######

##### Aim: a) Parse a sentence - "old men and women" and draw a tree using probabilistic parser #####

import nltk
from nltk import PCFG

grammar = PCFG.fromstring('''
NP -> NNS [0.5] | JJ NNS [0.3] | NP CC NP [0.2]
NNS -> "men" [0.1] | "women" [0.2] | "children" [0.3] | NNS CC NNS [0.4]
JJ -> "old" [0.4] | "young" [0.6]
CC -> "and" [0.9] | "or" [0.1]
''')

print(grammar)
viterbi_parser = nltk.ViterbiParser(grammar)
token = "old men and women".split()
obj = viterbi_parser.parse(token)

print("Output: ")
for x in obj:
    print(x)


##### Practical 8-B ######

##### Aim: Parse a sentence - 'I saw a bird from my window.' and draw a tree using malt parsing. #####

##### HINT:- #####

Set the environment variable -> System Variable -> New -> 
Variable Name:(MALT-PARSER) -> Variable Value:(C:\Users\lenovo\AppData\Local\Programs\Python\Python310\maltparser-1.7.2) 
Variable Name:(MALT-MODEL) -> Variable Value:(C:\Users\lenovo\AppData\Local\Programs\Python\Python310\ engmalt.linear-1.7.mco)
 
from nltk.parse import malt
mp = malt.MaltParser('maltparser-1.7.2','engmalt.linear-1.7.mco')
t = mp.parse_one('I saw a bird from my window.'.split()).tree()
print(t)
t.draw()





##### PRACTICAL NO.-9 ######

##### Practical 9-A #####

##### Aim: Multiword Expressions in NLP for multiword – ‘New Delhi’ in '''Good cake cost Rs.1500\kg in New Delhi.  Please buy me one of them.\n\nThanks.''' #####

import nltk
nltk.download('punkt')

#Multiword Expressions in NLP
from nltk.tokenize import MWETokenizer
from nltk import sent_tokenize, word_tokenize

s = '''Good cake cost Rs.1500\kg in New Delhi.  Please buy me one of them.\n\nThanks.'''
mwe = MWETokenizer([('New', 'Delhi'), ('New', 'York'), ('Hong', 'Kong')], separator='_')
for sent in sent_tokenize(s):
    print(mwe.tokenize(word_tokenize(sent)))



##### Practical 9-B #####

##### Aim: Word Sense Disambiguation for the keyword ‘jam’ in the sentences - 'This device is used to jam the signal' and 'I am stuck in a traffic jam'. Also, for the keyword ‘book’ in the sentences - 'I love reading books on coding.' and 'The table was already booked by someone else.' #####

from nltk.wsd import lesk
from nltk.tokenize import word_tokenize

import nltk
nltk.download('punkt')
nltk.download('wordnet')

a1= lesk(word_tokenize('This device is used to jam the signal'),'jam')
print(a1,a1.definition())
a2 = lesk(word_tokenize('I am stuck in a traffic jam'),'jam')
print(a2,a2.definition())

b1=lesk(word_tokenize('I love reading books on coding.'),'book')
print(b1,b1.definition())
b2 = lesk(word_tokenize('The table was already booked by someone else.'),'book')
print(b2,b2.definition())







#####  INDIAN LANGUAGES #####

##### Word Tokenization In Hindi #####

##### A #####

!pip install torch==2.0.1+cpu -f https://download.pytorch.org/whl/torch_stable.html
!pip install inltk
!pip install tornado==6.0.1
from inltk.inltk import setup
setup('hi')
from inltk.inltk import tokenize
hindi_text = """प्राकृ तिकभाषासीखनाबहुत तिलचस्पहै।"""
# tokenize(input text, language code)
tokenize(hindi_text, "hi")


##### B #####

!pip install torch==1.3.1+cpu -f https://download.pytorch.org/whl/torch_stable.html
!pip install inltk
!pip install tornado==4.5.3
from inltk.inltk import setupsetup('hi')
from inltk.inltk import get_similar_sentences
# get similar sentences to the one given in hindi
output = get_similar_sentences('मैं आज बहुि खुश ह ूं', 5, 'hi')
print(output)


##### C #####

!pip install torch==1.3.1+cpu -f https://download.pytorch.org/whl/torch_stable.html
!pip install inltk
!pip install tornado==4.5.3
from inltk.inltk import setup
setup('ml')
from inltk.inltk import identify_language
#Identify the detect Language of given text
identify_language('അഖിൽ')



##### DENOISING #####

import re

def remove_noise(text):
    # Remove special characters and digits
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    text = re.sub(r'\d+', '', text)
    return text

# Sample noisy text
noisy_text = "Hello! This is an example text with #@$! special characters and 12345 numbers."

# Remove noise from the text
clean_text = remove_noise(noisy_text)

# Print the original and cleaned text
print("Noisy Text: ", noisy_text)
print("Cleaned Text: ", clean_text)
