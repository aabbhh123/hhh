{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0af713f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dc4c81ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 419 files [00:05, 80.22 files/s] \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b878a342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61beda6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f687c2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Harshal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ids of brown corpus\n",
      " ['ca01', 'ca02', 'ca03', 'ca04', 'ca05', 'ca06', 'ca07', 'ca08', 'ca09', 'ca10', 'ca11', 'ca12', 'ca13', 'ca14', 'ca15', 'ca16', 'ca17', 'ca18', 'ca19', 'ca20', 'ca21', 'ca22', 'ca23', 'ca24', 'ca25', 'ca26', 'ca27', 'ca28', 'ca29', 'ca30', 'ca31', 'ca32', 'ca33', 'ca34', 'ca35', 'ca36', 'ca37', 'ca38', 'ca39', 'ca40', 'ca41', 'ca42', 'ca43', 'ca44', 'cb01', 'cb02', 'cb03', 'cb04', 'cb05', 'cb06', 'cb07', 'cb08', 'cb09', 'cb10', 'cb11', 'cb12', 'cb13', 'cb14', 'cb15', 'cb16', 'cb17', 'cb18', 'cb19', 'cb20', 'cb21', 'cb22', 'cb23', 'cb24', 'cb25', 'cb26', 'cb27', 'cc01', 'cc02', 'cc03', 'cc04', 'cc05', 'cc06', 'cc07', 'cc08', 'cc09', 'cc10', 'cc11', 'cc12', 'cc13', 'cc14', 'cc15', 'cc16', 'cc17', 'cd01', 'cd02', 'cd03', 'cd04', 'cd05', 'cd06', 'cd07', 'cd08', 'cd09', 'cd10', 'cd11', 'cd12', 'cd13', 'cd14', 'cd15', 'cd16', 'cd17', 'ce01', 'ce02', 'ce03', 'ce04', 'ce05', 'ce06', 'ce07', 'ce08', 'ce09', 'ce10', 'ce11', 'ce12', 'ce13', 'ce14', 'ce15', 'ce16', 'ce17', 'ce18', 'ce19', 'ce20', 'ce21', 'ce22', 'ce23', 'ce24', 'ce25', 'ce26', 'ce27', 'ce28', 'ce29', 'ce30', 'ce31', 'ce32', 'ce33', 'ce34', 'ce35', 'ce36', 'cf01', 'cf02', 'cf03', 'cf04', 'cf05', 'cf06', 'cf07', 'cf08', 'cf09', 'cf10', 'cf11', 'cf12', 'cf13', 'cf14', 'cf15', 'cf16', 'cf17', 'cf18', 'cf19', 'cf20', 'cf21', 'cf22', 'cf23', 'cf24', 'cf25', 'cf26', 'cf27', 'cf28', 'cf29', 'cf30', 'cf31', 'cf32', 'cf33', 'cf34', 'cf35', 'cf36', 'cf37', 'cf38', 'cf39', 'cf40', 'cf41', 'cf42', 'cf43', 'cf44', 'cf45', 'cf46', 'cf47', 'cf48', 'cg01', 'cg02', 'cg03', 'cg04', 'cg05', 'cg06', 'cg07', 'cg08', 'cg09', 'cg10', 'cg11', 'cg12', 'cg13', 'cg14', 'cg15', 'cg16', 'cg17', 'cg18', 'cg19', 'cg20', 'cg21', 'cg22', 'cg23', 'cg24', 'cg25', 'cg26', 'cg27', 'cg28', 'cg29', 'cg30', 'cg31', 'cg32', 'cg33', 'cg34', 'cg35', 'cg36', 'cg37', 'cg38', 'cg39', 'cg40', 'cg41', 'cg42', 'cg43', 'cg44', 'cg45', 'cg46', 'cg47', 'cg48', 'cg49', 'cg50', 'cg51', 'cg52', 'cg53', 'cg54', 'cg55', 'cg56', 'cg57', 'cg58', 'cg59', 'cg60', 'cg61', 'cg62', 'cg63', 'cg64', 'cg65', 'cg66', 'cg67', 'cg68', 'cg69', 'cg70', 'cg71', 'cg72', 'cg73', 'cg74', 'cg75', 'ch01', 'ch02', 'ch03', 'ch04', 'ch05', 'ch06', 'ch07', 'ch08', 'ch09', 'ch10', 'ch11', 'ch12', 'ch13', 'ch14', 'ch15', 'ch16', 'ch17', 'ch18', 'ch19', 'ch20', 'ch21', 'ch22', 'ch23', 'ch24', 'ch25', 'ch26', 'ch27', 'ch28', 'ch29', 'ch30', 'cj01', 'cj02', 'cj03', 'cj04', 'cj05', 'cj06', 'cj07', 'cj08', 'cj09', 'cj10', 'cj11', 'cj12', 'cj13', 'cj14', 'cj15', 'cj16', 'cj17', 'cj18', 'cj19', 'cj20', 'cj21', 'cj22', 'cj23', 'cj24', 'cj25', 'cj26', 'cj27', 'cj28', 'cj29', 'cj30', 'cj31', 'cj32', 'cj33', 'cj34', 'cj35', 'cj36', 'cj37', 'cj38', 'cj39', 'cj40', 'cj41', 'cj42', 'cj43', 'cj44', 'cj45', 'cj46', 'cj47', 'cj48', 'cj49', 'cj50', 'cj51', 'cj52', 'cj53', 'cj54', 'cj55', 'cj56', 'cj57', 'cj58', 'cj59', 'cj60', 'cj61', 'cj62', 'cj63', 'cj64', 'cj65', 'cj66', 'cj67', 'cj68', 'cj69', 'cj70', 'cj71', 'cj72', 'cj73', 'cj74', 'cj75', 'cj76', 'cj77', 'cj78', 'cj79', 'cj80', 'ck01', 'ck02', 'ck03', 'ck04', 'ck05', 'ck06', 'ck07', 'ck08', 'ck09', 'ck10', 'ck11', 'ck12', 'ck13', 'ck14', 'ck15', 'ck16', 'ck17', 'ck18', 'ck19', 'ck20', 'ck21', 'ck22', 'ck23', 'ck24', 'ck25', 'ck26', 'ck27', 'ck28', 'ck29', 'cl01', 'cl02', 'cl03', 'cl04', 'cl05', 'cl06', 'cl07', 'cl08', 'cl09', 'cl10', 'cl11', 'cl12', 'cl13', 'cl14', 'cl15', 'cl16', 'cl17', 'cl18', 'cl19', 'cl20', 'cl21', 'cl22', 'cl23', 'cl24', 'cm01', 'cm02', 'cm03', 'cm04', 'cm05', 'cm06', 'cn01', 'cn02', 'cn03', 'cn04', 'cn05', 'cn06', 'cn07', 'cn08', 'cn09', 'cn10', 'cn11', 'cn12', 'cn13', 'cn14', 'cn15', 'cn16', 'cn17', 'cn18', 'cn19', 'cn20', 'cn21', 'cn22', 'cn23', 'cn24', 'cn25', 'cn26', 'cn27', 'cn28', 'cn29', 'cp01', 'cp02', 'cp03', 'cp04', 'cp05', 'cp06', 'cp07', 'cp08', 'cp09', 'cp10', 'cp11', 'cp12', 'cp13', 'cp14', 'cp15', 'cp16', 'cp17', 'cp18', 'cp19', 'cp20', 'cp21', 'cp22', 'cp23', 'cp24', 'cp25', 'cp26', 'cp27', 'cp28', 'cp29', 'cr01', 'cr02', 'cr03', 'cr04', 'cr05', 'cr06', 'cr07', 'cr08', 'cr09']\n",
      "\n",
      "ca01 has following words:\n",
      " ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n",
      "\n",
      "ca01 has 2242 words\n",
      "\n",
      "\n",
      "Categories or file in brown corpus:\n",
      "\n",
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
      "\n",
      "\n",
      "Statistics for each text:\n",
      "\n",
      "AvgWordLen\tAvgSentenceLen\tno.ofTimesEachWordAppearsOnAvg\t\tFileName\n",
      "9 \t\t\t 22 \t\t\t 2 \t\t\t ca01\n",
      "8 \t\t\t 23 \t\t\t 2 \t\t\t ca02\n",
      "8 \t\t\t 20 \t\t\t 2 \t\t\t ca03\n",
      "9 \t\t\t 25 \t\t\t 2 \t\t\t ca04\n",
      "8 \t\t\t 26 \t\t\t 3 \t\t\t ca05\n",
      "8 \t\t\t 22 \t\t\t 2 \t\t\t ca06\n",
      "9 \t\t\t 18 \t\t\t 2 \t\t\t ca07\n",
      "8 \t\t\t 21 \t\t\t 2 \t\t\t ca08\n",
      "9 \t\t\t 19 \t\t\t 2 \t\t\t ca09\n",
      "8 \t\t\t 21 \t\t\t 2 \t\t\t ca10\n",
      "8 \t\t\t 22 \t\t\t 2 \t\t\t ca11\n",
      "8 \t\t\t 22 \t\t\t 2 \t\t\t ca12\n",
      "8 \t\t\t 20 \t\t\t 2 \t\t\t ca13\n",
      "8 \t\t\t 17 \t\t\t 2 \t\t\t ca14\n",
      "8 \t\t\t 21 \t\t\t 2 \t\t\t ca15\n",
      "8 \t\t\t 20 \t\t\t 2 \t\t\t ca16\n",
      "8 \t\t\t 22 \t\t\t 2 \t\t\t ca17\n",
      "8 \t\t\t 22 \t\t\t 2 \t\t\t ca18\n",
      "8 \t\t\t 20 \t\t\t 2 \t\t\t ca19\n",
      "8 \t\t\t 23 \t\t\t 2 \t\t\t ca20\n",
      "9 \t\t\t 20 \t\t\t 2 \t\t\t ca21\n",
      "8 \t\t\t 21 \t\t\t 2 \t\t\t ca22\n",
      "8 \t\t\t 20 \t\t\t 2 \t\t\t ca23\n",
      "8 \t\t\t 22 \t\t\t 2 \t\t\t ca24\n",
      "8 \t\t\t 21 \t\t\t 2 \t\t\t ca25\n",
      "8 \t\t\t 21 \t\t\t 2 \t\t\t ca26\n",
      "8 \t\t\t 19 \t\t\t 2 \t\t\t ca27\n",
      "8 \t\t\t 23 \t\t\t 2 \t\t\t ca28\n",
      "8 \t\t\t 19 \t\t\t 2 \t\t\t ca29\n",
      "8 \t\t\t 19 \t\t\t 2 \t\t\t ca30\n",
      "8 \t\t\t 20 \t\t\t 2 \t\t\t ca31\n",
      "8 \t\t\t 18 \t\t\t 2 \t\t\t ca32\n",
      "8 \t\t\t 24 \t\t\t 2 \t\t\t ca33\n",
      "8 \t\t\t 23 \t\t\t 3 \t\t\t ca34\n",
      "8 \t\t\t 29 \t\t\t 3 \t\t\t ca35\n",
      "9 \t\t\t 25 \t\t\t 2 \t\t\t ca36\n",
      "8 \t\t\t 24 \t\t\t 2 \t\t\t ca37\n",
      "8 \t\t\t 23 \t\t\t 3 \t\t\t ca38\n",
      "8 \t\t\t 19 \t\t\t 2 \t\t\t ca39\n",
      "8 \t\t\t 24 \t\t\t 2 \t\t\t ca40\n",
      "8 \t\t\t 22 \t\t\t 2 \t\t\t ca41\n",
      "8 \t\t\t 22 \t\t\t 2 \t\t\t ca42\n",
      "8 \t\t\t 17 \t\t\t 3 \t\t\t ca43\n",
      "8 \t\t\t 24 \t\t\t 2 \t\t\t ca44\n",
      "9 \t\t\t 21 \t\t\t 2 \t\t\t cb01\n",
      "9 \t\t\t 23 \t\t\t 2 \t\t\t cb02\n",
      "8 \t\t\t 23 \t\t\t 2 \t\t\t cb03\n",
      "8 \t\t\t 19 \t\t\t 2 \t\t\t cb04\n",
      "8 \t\t\t 21 \t\t\t 2 \t\t\t cb05\n",
      "9 \t\t\t 22 \t\t\t 2 \t\t\t cb06\n",
      "9 \t\t\t 24 \t\t\t 2 \t\t\t cb07\n",
      "8 \t\t\t 18 \t\t\t 3 \t\t\t cb08\n",
      "8 \t\t\t 18 \t\t\t 2 \t\t\t cb09\n",
      "8 \t\t\t 16 \t\t\t 3 \t\t\t cb10\n",
      "9 \t\t\t 20 \t\t\t 2 \t\t\t cb11\n",
      "8 \t\t\t 21 \t\t\t 2 \t\t\t cb12\n",
      "8 \t\t\t 15 \t\t\t 2 \t\t\t cb13\n",
      "8 \t\t\t 19 \t\t\t 2 \t\t\t cb14\n",
      "8 \t\t\t 22 \t\t\t 2 \t\t\t cb15\n",
      "8 \t\t\t 18 \t\t\t 2 \t\t\t cb16\n",
      "8 \t\t\t 18 \t\t\t 2 \t\t\t cb17\n",
      "9 \t\t\t 19 \t\t\t 2 \t\t\t cb18\n",
      "8 \t\t\t 16 \t\t\t 2 \t\t\t cb19\n",
      "8 \t\t\t 20 \t\t\t 2 \t\t\t cb20\n",
      "8 \t\t\t 29 \t\t\t 2 \t\t\t cb21\n",
      "8 \t\t\t 24 \t\t\t 2 \t\t\t cb22\n",
      "8 \t\t\t 17 \t\t\t 2 \t\t\t cb23\n",
      "8 \t\t\t 21 \t\t\t 2 \t\t\t cb24\n",
      "8 \t\t\t 29 \t\t\t 2 \t\t\t cb25\n",
      "8 \t\t\t 20 \t\t\t 2 \t\t\t cb26\n",
      "8 \t\t\t 20 \t\t\t 2 \t\t\t cb27\n",
      "8 \t\t\t 20 \t\t\t 2 \t\t\t cc01\n",
      "8 \t\t\t 23 \t\t\t 2 \t\t\t cc02\n",
      "8 \t\t\t 20 \t\t\t 2 \t\t\t cc03\n",
      "8 \t\t\t 22 \t\t\t 2 \t\t\t cc04\n",
      "8 \t\t\t 22 \t\t\t 2 \t\t\t cc05\n",
      "8 \t\t\t 29 \t\t\t 2 \t\t\t cc06\n",
      "8 \t\t\t 20 \t\t\t 2 \t\t\t cc07\n",
      "8 \t\t\t 25 \t\t\t 2 \t\t\t cc08\n",
      "8 \t\t\t 26 \t\t\t 2 \t\t\t cc09\n",
      "8 \t\t\t 22 \t\t\t 2 \t\t\t cc10\n",
      "8 \t\t\t 22 \t\t\t 2 \t\t\t cc11\n",
      "8 \t\t\t 25 \t\t\t 2 \t\t\t cc12\n",
      "8 \t\t\t 24 \t\t\t 2 \t\t\t cc13\n",
      "8 \t\t\t 25 \t\t\t 2 \t\t\t cc14\n",
      "8 \t\t\t 19 \t\t\t 2 \t\t\t cc15\n",
      "8 \t\t\t 24 \t\t\t 2 \t\t\t cc16\n",
      "8 \t\t\t 23 \t\t\t 2 \t\t\t cc17\n",
      "8 \t\t\t 26 \t\t\t 3 \t\t\t cd01\n",
      "8 \t\t\t 34 \t\t\t 3 \t\t\t cd02\n",
      "8 \t\t\t 27 \t\t\t 2 \t\t\t cd03\n",
      "8 \t\t\t 24 \t\t\t 3 \t\t\t cd04\n",
      "8 \t\t\t 33 \t\t\t 2 \t\t\t cd05\n",
      "8 \t\t\t 26 \t\t\t 3 \t\t\t cd06\n",
      "8 \t\t\t 19 \t\t\t 3 \t\t\t cd07\n",
      "8 \t\t\t 32 \t\t\t 3 \t\t\t cd08\n",
      "8 \t\t\t 13 \t\t\t 3 \t\t\t cd09\n",
      "9 \t\t\t 23 \t\t\t 2 \t\t\t cd10\n",
      "8 \t\t\t 34 \t\t\t 3 \t\t\t cd11\n",
      "9 \t\t\t 28 \t\t\t 2 \t\t\t cd12\n",
      "8 \t\t\t 21 \t\t\t 3 \t\t\t cd13\n",
      "9 \t\t\t 24 \t\t\t 2 \t\t\t cd14\n",
      "8 \t\t\t 23 \t\t\t 2 \t\t\t cd15\n",
      "8 \t\t\t 14 \t\t\t 4 \t\t\t cd16\n",
      "8 \t\t\t 15 \t\t\t 2 \t\t\t cd17\n",
      "8 \t\t\t 21 \t\t\t 2 \t\t\t ce01\n",
      "8 \t\t\t 18 \t\t\t 2 \t\t\t ce02\n",
      "8 \t\t\t 22 \t\t\t 3 \t\t\t ce03\n",
      "8 \t\t\t 23 \t\t\t 2 \t\t\t ce04\n",
      "8 \t\t\t 20 \t\t\t 3 \t\t\t ce05\n",
      "8 \t\t\t 23 \t\t\t 2 \t\t\t ce06\n",
      "8 \t\t\t 17 \t\t\t 3 \t\t\t ce07\n",
      "8 \t\t\t 17 \t\t\t 4 \t\t\t ce08\n",
      "8 \t\t\t 17 \t\t\t 3 \t\t\t ce09\n",
      "8 \t\t\t 18 \t\t\t 2 \t\t\t ce10\n",
      "8 \t\t\t 20 \t\t\t 2 \t\t\t ce11\n",
      "9 \t\t\t 18 \t\t\t 2 \t\t\t ce12\n",
      "8 \t\t\t 30 \t\t\t 3 \t\t\t ce13\n",
      "8 \t\t\t 13 \t\t\t 2 \t\t\t ce14\n",
      "8 \t\t\t 11 \t\t\t 3 \t\t\t ce15\n",
      "8 \t\t\t 17 \t\t\t 3 \t\t\t ce16\n",
      "8 \t\t\t 20 \t\t\t 3 \t\t\t ce17\n",
      "8 \t\t\t 27 \t\t\t 2 \t\t\t ce18\n",
      "8 \t\t\t 20 \t\t\t 2 \t\t\t ce19\n",
      "8 \t\t\t 18 \t\t\t 3 \t\t\t ce20\n",
      "8 \t\t\t 19 \t\t\t 3 \t\t\t ce21\n",
      "8 \t\t\t 22 \t\t\t 2 \t\t\t ce22\n",
      "8 \t\t\t 25 \t\t\t 2 \t\t\t ce23\n",
      "8 \t\t\t 18 \t\t\t 3 \t\t\t ce24\n",
      "9 \t\t\t 23 \t\t\t 2 \t\t\t ce25\n",
      "8 \t\t\t 24 \t\t\t 2 \t\t\t ce26\n",
      "8 \t\t\t 12 \t\t\t 4 \t\t\t ce27\n",
      "9 \t\t\t 18 \t\t\t 3 \t\t\t ce28\n",
      "8 \t\t\t 25 \t\t\t 3 \t\t\t ce29\n",
      "9 \t\t\t 15 \t\t\t 2 \t\t\t ce30\n",
      "9 \t\t\t 21 \t\t\t 2 \t\t\t ce31\n",
      "9 \t\t\t 19 \t\t\t 3 \t\t\t ce32\n",
      "8 \t\t\t 19 \t\t\t 2 \t\t\t ce33\n",
      "8 \t\t\t 21 \t\t\t 2 \t\t\t ce34\n",
      "8 \t\t\t 20 \t\t\t 3 \t\t\t ce35\n",
      "8 \t\t\t 29 \t\t\t 3 \t\t\t ce36\n",
      "8 \t\t\t 32 \t\t\t 2 \t\t\t cf01\n",
      "8 \t\t\t 18 \t\t\t 2 \t\t\t cf02\n",
      "8 \t\t\t 30 \t\t\t 2 \t\t\t cf03\n",
      "8 \t\t\t 18 \t\t\t 2 \t\t\t cf04\n",
      "8 \t\t\t 25 \t\t\t 2 \t\t\t cf05\n",
      "8 \t\t\t 14 \t\t\t 3 \t\t\t cf06\n",
      "8 \t\t\t 24 \t\t\t 2 \t\t\t cf07\n",
      "8 \t\t\t 20 \t\t\t 2 \t\t\t cf08\n",
      "8 \t\t\t 17 \t\t\t 2 \t\t\t cf09\n",
      "8 \t\t\t 22 \t\t\t 2 \t\t\t cf10\n",
      "8 \t\t\t 19 \t\t\t 3 \t\t\t cf11\n",
      "8 \t\t\t 24 \t\t\t 3 \t\t\t cf12\n",
      "8 \t\t\t 16 \t\t\t 3 \t\t\t cf13\n",
      "8 \t\t\t 32 \t\t\t 2 \t\t\t cf14\n",
      "9 \t\t\t 27 \t\t\t 2 \t\t\t cf15\n",
      "8 \t\t\t 18 \t\t\t 2 \t\t\t cf16\n",
      "8 \t\t\t 30 \t\t\t 2 \t\t\t cf17\n",
      "8 \t\t\t 23 \t\t\t 2 \t\t\t cf18\n",
      "8 \t\t\t 26 \t\t\t 2 \t\t\t cf19\n",
      "8 \t\t\t 25 \t\t\t 2 \t\t\t cf20\n",
      "8 \t\t\t 20 \t\t\t 2 \t\t\t cf21\n",
      "8 \t\t\t 20 \t\t\t 3 \t\t\t cf22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 \t\t\t 22 \t\t\t 2 \t\t\t cf23\n",
      "8 \t\t\t 16 \t\t\t 2 \t\t\t cf24\n",
      "8 \t\t\t 21 \t\t\t 2 \t\t\t cf25\n",
      "8 \t\t\t 18 \t\t\t 3 \t\t\t cf26\n",
      "8 \t\t\t 26 \t\t\t 3 \t\t\t cf27\n",
      "8 \t\t\t 23 \t\t\t 2 \t\t\t cf28\n",
      "8 \t\t\t 23 \t\t\t 2 \t\t\t cf29\n",
      "8 \t\t\t 23 \t\t\t 2 \t\t\t cf30\n",
      "8 \t\t\t 21 \t\t\t 3 \t\t\t cf31\n",
      "8 \t\t\t 20 \t\t\t 2 \t\t\t cf32\n",
      "8 \t\t\t 24 \t\t\t 3 \t\t\t cf33\n",
      "8 \t\t\t 20 \t\t\t 3 \t\t\t cf34\n",
      "8 \t\t\t 24 \t\t\t 3 \t\t\t cf35\n",
      "8 \t\t\t 18 \t\t\t 3 \t\t\t cf36\n",
      "8 \t\t\t 23 \t\t\t 3 \t\t\t cf37\n",
      "8 \t\t\t 40 \t\t\t 2 \t\t\t cf38\n",
      "9 \t\t\t 30 \t\t\t 3 \t\t\t cf39\n",
      "9 \t\t\t 26 \t\t\t 2 \t\t\t cf40\n",
      "8 \t\t\t 22 \t\t\t 3 \t\t\t cf41\n",
      "8 \t\t\t 22 \t\t\t 3 \t\t\t cf42\n",
      "8 \t\t\t 23 \t\t\t 3 \t\t\t cf43\n",
      "9 \t\t\t 23 \t\t\t 3 \t\t\t cf44\n",
      "8 \t\t\t 22 \t\t\t 3 \t\t\t cf45\n",
      "9 \t\t\t 20 \t\t\t 2 \t\t\t cf46\n",
      "8 \t\t\t 21 \t\t\t 3 \t\t\t cf47\n",
      "8 \t\t\t 38 \t\t\t 3 \t\t\t cf48\n",
      "8 \t\t\t 22 \t\t\t 2 \t\t\t cg01\n",
      "9 \t\t\t 25 \t\t\t 2 \t\t\t cg02\n",
      "8 \t\t\t 19 \t\t\t 2 \t\t\t cg03\n",
      "8 \t\t\t 16 \t\t\t 3 \t\t\t cg04\n",
      "8 \t\t\t 24 \t\t\t 2 \t\t\t cg05\n",
      "8 \t\t\t 22 \t\t\t 2 \t\t\t cg06\n",
      "9 \t\t\t 26 \t\t\t 2 \t\t\t cg07\n",
      "8 \t\t\t 26 \t\t\t 2 \t\t\t cg08\n",
      "8 \t\t\t 21 \t\t\t 2 \t\t\t cg09\n",
      "8 \t\t\t 32 \t\t\t 3 \t\t\t cg10\n",
      "8 \t\t\t 24 \t\t\t 2 \t\t\t cg11\n",
      "8 \t\t\t 16 \t\t\t 3 \t\t\t cg12\n",
      "8 \t\t\t 22 \t\t\t 2 \t\t\t cg13\n",
      "8 \t\t\t 23 \t\t\t 2 \t\t\t cg14\n",
      "8 \t\t\t 27 \t\t\t 3 \t\t\t cg15\n",
      "8 \t\t\t 30 \t\t\t 3 \t\t\t cg16\n",
      "8 \t\t\t 22 \t\t\t 2 \t\t\t cg17\n",
      "8 \t\t\t 21 \t\t\t 2 \t\t\t cg18\n",
      "8 \t\t\t 19 \t\t\t 2 \t\t\t cg19\n",
      "8 \t\t\t 25 \t\t\t 3 \t\t\t cg20\n",
      "8 \t\t\t 25 \t\t\t 2 \t\t\t cg21\n",
      "8 \t\t\t 24 \t\t\t 2 \t\t\t cg22\n",
      "8 \t\t\t 18 \t\t\t 2 \t\t\t cg23\n",
      "8 \t\t\t 30 \t\t\t 2 \t\t\t cg24\n",
      "8 \t\t\t 21 \t\t\t 3 \t\t\t cg25\n",
      "8 \t\t\t 25 \t\t\t 3 \t\t\t cg26\n",
      "8 \t\t\t 23 \t\t\t 2 \t\t\t cg27\n",
      "8 \t\t\t 34 \t\t\t 2 \t\t\t cg28\n",
      "8 \t\t\t 28 \t\t\t 2 \t\t\t cg29\n",
      "8 \t\t\t 19 \t\t\t 2 \t\t\t cg30\n",
      "8 \t\t\t 32 \t\t\t 2 \t\t\t cg31\n",
      "8 \t\t\t 22 \t\t\t 2 \t\t\t cg32\n",
      "8 \t\t\t 31 \t\t\t 2 \t\t\t cg33\n",
      "8 \t\t\t 19 \t\t\t 2 \t\t\t cg34\n",
      "8 \t\t\t 22 \t\t\t 2 \t\t\t cg35\n",
      "8 \t\t\t 16 \t\t\t 3 \t\t\t cg36\n",
      "8 \t\t\t 20 \t\t\t 2 \t\t\t cg37\n",
      "8 \t\t\t 30 \t\t\t 3 \t\t\t cg38\n",
      "8 \t\t\t 24 \t\t\t 2 \t\t\t cg39\n",
      "8 \t\t\t 19 \t\t\t 2 \t\t\t cg40\n",
      "8 \t\t\t 20 \t\t\t 3 \t\t\t cg41\n",
      "9 \t\t\t 22 \t\t\t 2 \t\t\t cg42\n",
      "8 \t\t\t 29 \t\t\t 3 \t\t\t cg43\n",
      "8 \t\t\t 31 \t\t\t 3 \t\t\t cg44\n",
      "8 \t\t\t 23 \t\t\t 2 \t\t\t cg45\n",
      "9 \t\t\t 26 \t\t\t 2 \t\t\t cg46\n",
      "8 \t\t\t 18 \t\t\t 3 \t\t\t cg47\n",
      "8 \t\t\t 21 \t\t\t 2 \t\t\t cg48\n",
      "8 \t\t\t 28 \t\t\t 3 \t\t\t cg49\n",
      "8 \t\t\t 22 \t\t\t 2 \t\t\t cg50\n",
      "8 \t\t\t 21 \t\t\t 2 \t\t\t cg51\n",
      "8 \t\t\t 26 \t\t\t 2 \t\t\t cg52\n",
      "8 \t\t\t 22 \t\t\t 2 \t\t\t cg53\n",
      "8 \t\t\t 19 \t\t\t 3 \t\t\t cg54\n",
      "8 \t\t\t 20 \t\t\t 3 \t\t\t cg55\n",
      "8 \t\t\t 26 \t\t\t 3 \t\t\t cg56\n",
      "8 \t\t\t 27 \t\t\t 2 \t\t\t cg57\n",
      "8 \t\t\t 30 \t\t\t 2 \t\t\t cg58\n",
      "8 \t\t\t 46 \t\t\t 2 \t\t\t cg59\n",
      "8 \t\t\t 18 \t\t\t 3 \t\t\t cg60\n",
      "9 \t\t\t 27 \t\t\t 2 \t\t\t cg61\n",
      "8 \t\t\t 27 \t\t\t 2 \t\t\t cg62\n",
      "8 \t\t\t 20 \t\t\t 2 \t\t\t cg63\n",
      "8 \t\t\t 33 \t\t\t 2 \t\t\t cg64\n",
      "8 \t\t\t 24 \t\t\t 2 \t\t\t cg65\n",
      "8 \t\t\t 32 \t\t\t 2 \t\t\t cg66\n",
      "8 \t\t\t 28 \t\t\t 3 \t\t\t cg67\n",
      "8 \t\t\t 29 \t\t\t 2 \t\t\t cg68\n",
      "8 \t\t\t 42 \t\t\t 2 \t\t\t cg69\n",
      "8 \t\t\t 25 \t\t\t 3 \t\t\t cg70\n",
      "8 \t\t\t 31 \t\t\t 2 \t\t\t cg71\n",
      "8 \t\t\t 27 \t\t\t 3 \t\t\t cg72\n",
      "8 \t\t\t 23 \t\t\t 3 \t\t\t cg73\n",
      "8 \t\t\t 24 \t\t\t 2 \t\t\t cg74\n",
      "8 \t\t\t 17 \t\t\t 3 \t\t\t cg75\n",
      "9 \t\t\t 16 \t\t\t 3 \t\t\t ch01\n",
      "9 \t\t\t 20 \t\t\t 3 \t\t\t ch02\n",
      "8 \t\t\t 26 \t\t\t 3 \t\t\t ch03\n",
      "9 \t\t\t 18 \t\t\t 3 \t\t\t ch04\n",
      "8 \t\t\t 26 \t\t\t 3 \t\t\t ch05\n",
      "9 \t\t\t 22 \t\t\t 2 \t\t\t ch06\n",
      "8 \t\t\t 18 \t\t\t 3 \t\t\t ch07\n",
      "9 \t\t\t 28 \t\t\t 3 \t\t\t ch08\n",
      "8 \t\t\t 23 \t\t\t 3 \t\t\t ch09\n",
      "9 \t\t\t 21 \t\t\t 3 \t\t\t ch10\n",
      "9 \t\t\t 22 \t\t\t 2 \t\t\t ch11\n",
      "8 \t\t\t 30 \t\t\t 5 \t\t\t ch12\n",
      "8 \t\t\t 22 \t\t\t 2 \t\t\t ch13\n",
      "8 \t\t\t 21 \t\t\t 5 \t\t\t ch14\n",
      "8 \t\t\t 14 \t\t\t 3 \t\t\t ch15\n",
      "8 \t\t\t 29 \t\t\t 3 \t\t\t ch16\n",
      "8 \t\t\t 23 \t\t\t 3 \t\t\t ch17\n",
      "8 \t\t\t 26 \t\t\t 3 \t\t\t ch18\n",
      "9 \t\t\t 23 \t\t\t 3 \t\t\t ch19\n",
      "9 \t\t\t 22 \t\t\t 3 \t\t\t ch20\n",
      "9 \t\t\t 25 \t\t\t 3 \t\t\t ch21\n",
      "9 \t\t\t 30 \t\t\t 4 \t\t\t ch22\n",
      "9 \t\t\t 32 \t\t\t 4 \t\t\t ch23\n",
      "8 \t\t\t 19 \t\t\t 4 \t\t\t ch24\n",
      "8 \t\t\t 28 \t\t\t 3 \t\t\t ch25\n",
      "9 \t\t\t 31 \t\t\t 3 \t\t\t ch26\n",
      "9 \t\t\t 20 \t\t\t 2 \t\t\t ch27\n",
      "9 \t\t\t 19 \t\t\t 3 \t\t\t ch28\n",
      "9 \t\t\t 30 \t\t\t 3 \t\t\t ch29\n",
      "9 \t\t\t 22 \t\t\t 2 \t\t\t ch30\n",
      "8 \t\t\t 30 \t\t\t 3 \t\t\t cj01\n",
      "8 \t\t\t 19 \t\t\t 3 \t\t\t cj02\n",
      "8 \t\t\t 26 \t\t\t 3 \t\t\t cj03\n",
      "8 \t\t\t 20 \t\t\t 3 \t\t\t cj04\n",
      "9 \t\t\t 23 \t\t\t 3 \t\t\t cj05\n",
      "9 \t\t\t 26 \t\t\t 3 \t\t\t cj06\n",
      "8 \t\t\t 23 \t\t\t 3 \t\t\t cj07\n",
      "9 \t\t\t 19 \t\t\t 2 \t\t\t cj08\n",
      "8 \t\t\t 23 \t\t\t 3 \t\t\t cj09\n",
      "8 \t\t\t 23 \t\t\t 3 \t\t\t cj10\n",
      "8 \t\t\t 22 \t\t\t 3 \t\t\t cj11\n",
      "8 \t\t\t 22 \t\t\t 3 \t\t\t cj12\n",
      "8 \t\t\t 30 \t\t\t 4 \t\t\t cj13\n",
      "9 \t\t\t 21 \t\t\t 3 \t\t\t cj14\n",
      "9 \t\t\t 20 \t\t\t 2 \t\t\t cj15\n",
      "8 \t\t\t 24 \t\t\t 4 \t\t\t cj16\n",
      "9 \t\t\t 26 \t\t\t 3 \t\t\t cj17\n",
      "7 \t\t\t 18 \t\t\t 6 \t\t\t cj18\n",
      "8 \t\t\t 18 \t\t\t 4 \t\t\t cj19\n",
      "8 \t\t\t 20 \t\t\t 5 \t\t\t cj20\n",
      "8 \t\t\t 28 \t\t\t 7 \t\t\t cj21\n",
      "9 \t\t\t 25 \t\t\t 2 \t\t\t cj22\n",
      "8 \t\t\t 21 \t\t\t 3 \t\t\t cj23\n",
      "9 \t\t\t 34 \t\t\t 2 \t\t\t cj24\n",
      "9 \t\t\t 19 \t\t\t 3 \t\t\t cj25\n",
      "9 \t\t\t 22 \t\t\t 3 \t\t\t cj26\n",
      "8 \t\t\t 23 \t\t\t 3 \t\t\t cj27\n",
      "9 \t\t\t 25 \t\t\t 3 \t\t\t cj28\n",
      "9 \t\t\t 23 \t\t\t 3 \t\t\t cj29\n",
      "8 \t\t\t 21 \t\t\t 3 \t\t\t cj30\n",
      "8 \t\t\t 38 \t\t\t 3 \t\t\t cj31\n",
      "8 \t\t\t 21 \t\t\t 4 \t\t\t cj32\n",
      "9 \t\t\t 23 \t\t\t 4 \t\t\t cj33\n",
      "9 \t\t\t 21 \t\t\t 3 \t\t\t cj34\n",
      "8 \t\t\t 23 \t\t\t 3 \t\t\t cj35\n",
      "8 \t\t\t 27 \t\t\t 2 \t\t\t cj36\n",
      "9 \t\t\t 26 \t\t\t 2 \t\t\t cj37\n",
      "9 \t\t\t 22 \t\t\t 3 \t\t\t cj38\n",
      "8 \t\t\t 24 \t\t\t 2 \t\t\t cj39\n",
      "8 \t\t\t 31 \t\t\t 3 \t\t\t cj40\n",
      "8 \t\t\t 26 \t\t\t 3 \t\t\t cj41\n",
      "8 \t\t\t 25 \t\t\t 3 \t\t\t cj42\n",
      "8 \t\t\t 23 \t\t\t 3 \t\t\t cj43\n",
      "9 \t\t\t 28 \t\t\t 3 \t\t\t cj44\n",
      "9 \t\t\t 20 \t\t\t 3 \t\t\t cj45\n",
      "8 \t\t\t 25 \t\t\t 3 \t\t\t cj46\n",
      "8 \t\t\t 19 \t\t\t 3 \t\t\t cj47\n",
      "8 \t\t\t 20 \t\t\t 3 \t\t\t cj48\n",
      "9 \t\t\t 27 \t\t\t 3 \t\t\t cj49\n",
      "8 \t\t\t 33 \t\t\t 3 \t\t\t cj50\n",
      "8 \t\t\t 22 \t\t\t 2 \t\t\t cj51\n",
      "8 \t\t\t 23 \t\t\t 4 \t\t\t cj52\n",
      "8 \t\t\t 32 \t\t\t 3 \t\t\t cj53\n",
      "8 \t\t\t 23 \t\t\t 2 \t\t\t cj54\n",
      "8 \t\t\t 25 \t\t\t 2 \t\t\t cj55\n",
      "8 \t\t\t 22 \t\t\t 2 \t\t\t cj56\n",
      "8 \t\t\t 25 \t\t\t 3 \t\t\t cj57\n",
      "8 \t\t\t 27 \t\t\t 3 \t\t\t cj58\n",
      "8 \t\t\t 31 \t\t\t 3 \t\t\t cj59\n",
      "8 \t\t\t 28 \t\t\t 2 \t\t\t cj60\n",
      "8 \t\t\t 26 \t\t\t 2 \t\t\t cj61\n",
      "8 \t\t\t 25 \t\t\t 2 \t\t\t cj62\n",
      "8 \t\t\t 26 \t\t\t 2 \t\t\t cj63\n",
      "8 \t\t\t 20 \t\t\t 2 \t\t\t cj64\n",
      "8 \t\t\t 29 \t\t\t 3 \t\t\t cj65\n",
      "8 \t\t\t 20 \t\t\t 2 \t\t\t cj66\n",
      "8 \t\t\t 28 \t\t\t 2 \t\t\t cj67\n",
      "8 \t\t\t 40 \t\t\t 2 \t\t\t cj68\n",
      "8 \t\t\t 22 \t\t\t 4 \t\t\t cj69\n",
      "8 \t\t\t 18 \t\t\t 3 \t\t\t cj70\n",
      "9 \t\t\t 18 \t\t\t 3 \t\t\t cj71\n",
      "9 \t\t\t 21 \t\t\t 2 \t\t\t cj72\n",
      "9 \t\t\t 13 \t\t\t 3 \t\t\t cj73\n",
      "9 \t\t\t 21 \t\t\t 3 \t\t\t cj74\n",
      "8 \t\t\t 24 \t\t\t 4 \t\t\t cj75\n",
      "8 \t\t\t 23 \t\t\t 2 \t\t\t cj76\n",
      "9 \t\t\t 20 \t\t\t 3 \t\t\t cj77\n",
      "8 \t\t\t 19 \t\t\t 3 \t\t\t cj78\n",
      "8 \t\t\t 23 \t\t\t 4 \t\t\t cj79\n",
      "8 \t\t\t 19 \t\t\t 4 \t\t\t cj80\n",
      "8 \t\t\t 14 \t\t\t 3 \t\t\t ck01\n",
      "8 \t\t\t 16 \t\t\t 2 \t\t\t ck02\n",
      "8 \t\t\t 16 \t\t\t 2 \t\t\t ck03\n",
      "8 \t\t\t 17 \t\t\t 3 \t\t\t ck04\n",
      "8 \t\t\t 13 \t\t\t 3 \t\t\t ck05\n",
      "8 \t\t\t 12 \t\t\t 2 \t\t\t ck06\n",
      "7 \t\t\t 10 \t\t\t 3 \t\t\t ck07\n",
      "8 \t\t\t 16 \t\t\t 3 \t\t\t ck08\n",
      "8 \t\t\t 24 \t\t\t 3 \t\t\t ck09\n",
      "8 \t\t\t 15 \t\t\t 2 \t\t\t ck10\n",
      "7 \t\t\t 15 \t\t\t 3 \t\t\t ck11\n",
      "8 \t\t\t 17 \t\t\t 2 \t\t\t ck12\n",
      "8 \t\t\t 17 \t\t\t 3 \t\t\t ck13\n",
      "8 \t\t\t 16 \t\t\t 2 \t\t\t ck14\n",
      "8 \t\t\t 18 \t\t\t 2 \t\t\t ck15\n",
      "8 \t\t\t 18 \t\t\t 2 \t\t\t ck16\n",
      "8 \t\t\t 12 \t\t\t 3 \t\t\t ck17\n",
      "8 \t\t\t 14 \t\t\t 3 \t\t\t ck18\n",
      "8 \t\t\t 19 \t\t\t 3 \t\t\t ck19\n",
      "8 \t\t\t 17 \t\t\t 2 \t\t\t ck20\n",
      "8 \t\t\t 11 \t\t\t 2 \t\t\t ck21\n",
      "8 \t\t\t 17 \t\t\t 3 \t\t\t ck22\n",
      "8 \t\t\t 21 \t\t\t 2 \t\t\t ck23\n",
      "7 \t\t\t 10 \t\t\t 4 \t\t\t ck24\n",
      "8 \t\t\t 33 \t\t\t 2 \t\t\t ck25\n",
      "7 \t\t\t 17 \t\t\t 3 \t\t\t ck26\n",
      "8 \t\t\t 24 \t\t\t 2 \t\t\t ck27\n",
      "7 \t\t\t 13 \t\t\t 3 \t\t\t ck28\n",
      "8 \t\t\t 26 \t\t\t 2 \t\t\t ck29\n",
      "8 \t\t\t 15 \t\t\t 3 \t\t\t cl01\n",
      "8 \t\t\t 16 \t\t\t 3 \t\t\t cl02\n",
      "7 \t\t\t 12 \t\t\t 3 \t\t\t cl03\n",
      "8 \t\t\t 13 \t\t\t 3 \t\t\t cl04\n",
      "8 \t\t\t 15 \t\t\t 3 \t\t\t cl05\n",
      "8 \t\t\t 15 \t\t\t 3 \t\t\t cl06\n",
      "8 \t\t\t 16 \t\t\t 3 \t\t\t cl07\n",
      "7 \t\t\t 17 \t\t\t 3 \t\t\t cl08\n",
      "8 \t\t\t 14 \t\t\t 3 \t\t\t cl09\n",
      "8 \t\t\t 13 \t\t\t 3 \t\t\t cl10\n",
      "8 \t\t\t 15 \t\t\t 3 \t\t\t cl11\n",
      "8 \t\t\t 12 \t\t\t 2 \t\t\t cl12\n",
      "8 \t\t\t 13 \t\t\t 3 \t\t\t cl13\n",
      "7 \t\t\t 12 \t\t\t 3 \t\t\t cl14\n",
      "8 \t\t\t 15 \t\t\t 2 \t\t\t cl15\n",
      "8 \t\t\t 13 \t\t\t 3 \t\t\t cl16\n",
      "8 \t\t\t 19 \t\t\t 2 \t\t\t cl17\n",
      "8 \t\t\t 17 \t\t\t 3 \t\t\t cl18\n",
      "8 \t\t\t 15 \t\t\t 3 \t\t\t cl19\n",
      "7 \t\t\t 14 \t\t\t 3 \t\t\t cl20\n",
      "8 \t\t\t 11 \t\t\t 3 \t\t\t cl21\n",
      "8 \t\t\t 16 \t\t\t 3 \t\t\t cl22\n",
      "8 \t\t\t 14 \t\t\t 3 \t\t\t cl23\n",
      "7 \t\t\t 14 \t\t\t 3 \t\t\t cl24\n",
      "8 \t\t\t 14 \t\t\t 3 \t\t\t cm01\n",
      "8 \t\t\t 15 \t\t\t 3 \t\t\t cm02\n",
      "8 \t\t\t 16 \t\t\t 3 \t\t\t cm03\n",
      "8 \t\t\t 12 \t\t\t 3 \t\t\t cm04\n",
      "8 \t\t\t 19 \t\t\t 2 \t\t\t cm05\n",
      "8 \t\t\t 14 \t\t\t 3 \t\t\t cm06\n",
      "7 \t\t\t 14 \t\t\t 3 \t\t\t cn01\n",
      "7 \t\t\t 13 \t\t\t 3 \t\t\t cn02\n",
      "8 \t\t\t 11 \t\t\t 3 \t\t\t cn03\n",
      "8 \t\t\t 13 \t\t\t 3 \t\t\t cn04\n",
      "7 \t\t\t 12 \t\t\t 3 \t\t\t cn05\n",
      "8 \t\t\t 17 \t\t\t 3 \t\t\t cn06\n",
      "8 \t\t\t 11 \t\t\t 2 \t\t\t cn07\n",
      "8 \t\t\t 20 \t\t\t 2 \t\t\t cn08\n",
      "8 \t\t\t 13 \t\t\t 3 \t\t\t cn09\n",
      "8 \t\t\t 12 \t\t\t 3 \t\t\t cn10\n",
      "8 \t\t\t 18 \t\t\t 2 \t\t\t cn11\n",
      "8 \t\t\t 13 \t\t\t 3 \t\t\t cn12\n",
      "8 \t\t\t 19 \t\t\t 2 \t\t\t cn13\n",
      "8 \t\t\t 14 \t\t\t 2 \t\t\t cn14\n",
      "8 \t\t\t 13 \t\t\t 3 \t\t\t cn15\n",
      "8 \t\t\t 18 \t\t\t 2 \t\t\t cn16\n",
      "7 \t\t\t 13 \t\t\t 3 \t\t\t cn17\n",
      "8 \t\t\t 18 \t\t\t 3 \t\t\t cn18\n",
      "8 \t\t\t 16 \t\t\t 3 \t\t\t cn19\n",
      "8 \t\t\t 14 \t\t\t 2 \t\t\t cn20\n",
      "8 \t\t\t 16 \t\t\t 2 \t\t\t cn21\n",
      "8 \t\t\t 16 \t\t\t 3 \t\t\t cn22\n",
      "8 \t\t\t 18 \t\t\t 3 \t\t\t cn23\n",
      "7 \t\t\t 14 \t\t\t 3 \t\t\t cn24\n",
      "8 \t\t\t 12 \t\t\t 3 \t\t\t cn25\n",
      "8 \t\t\t 18 \t\t\t 2 \t\t\t cn26\n",
      "8 \t\t\t 19 \t\t\t 3 \t\t\t cn27\n",
      "8 \t\t\t 18 \t\t\t 2 \t\t\t cn28\n",
      "7 \t\t\t 13 \t\t\t 3 \t\t\t cn29\n",
      "8 \t\t\t 17 \t\t\t 3 \t\t\t cp01\n",
      "8 \t\t\t 23 \t\t\t 3 \t\t\t cp02\n",
      "8 \t\t\t 16 \t\t\t 3 \t\t\t cp03\n",
      "8 \t\t\t 19 \t\t\t 3 \t\t\t cp04\n",
      "8 \t\t\t 17 \t\t\t 2 \t\t\t cp05\n",
      "8 \t\t\t 13 \t\t\t 3 \t\t\t cp06\n",
      "8 \t\t\t 16 \t\t\t 3 \t\t\t cp07\n",
      "8 \t\t\t 13 \t\t\t 3 \t\t\t cp08\n",
      "8 \t\t\t 31 \t\t\t 3 \t\t\t cp09\n",
      "8 \t\t\t 19 \t\t\t 3 \t\t\t cp10\n",
      "8 \t\t\t 19 \t\t\t 3 \t\t\t cp11\n",
      "7 \t\t\t 14 \t\t\t 3 \t\t\t cp12\n",
      "8 \t\t\t 20 \t\t\t 3 \t\t\t cp13\n",
      "7 \t\t\t 10 \t\t\t 3 \t\t\t cp14\n",
      "7 \t\t\t 14 \t\t\t 3 \t\t\t cp15\n",
      "8 \t\t\t 12 \t\t\t 3 \t\t\t cp16\n",
      "8 \t\t\t 19 \t\t\t 3 \t\t\t cp17\n",
      "7 \t\t\t 13 \t\t\t 3 \t\t\t cp18\n",
      "7 \t\t\t 14 \t\t\t 3 \t\t\t cp19\n",
      "7 \t\t\t 13 \t\t\t 3 \t\t\t cp20\n",
      "8 \t\t\t 21 \t\t\t 3 \t\t\t cp21\n",
      "8 \t\t\t 13 \t\t\t 3 \t\t\t cp22\n",
      "8 \t\t\t 14 \t\t\t 3 \t\t\t cp23\n",
      "7 \t\t\t 10 \t\t\t 4 \t\t\t cp24\n",
      "8 \t\t\t 23 \t\t\t 3 \t\t\t cp25\n",
      "7 \t\t\t 21 \t\t\t 4 \t\t\t cp26\n",
      "8 \t\t\t 11 \t\t\t 3 \t\t\t cp27\n",
      "8 \t\t\t 14 \t\t\t 3 \t\t\t cp28\n",
      "8 \t\t\t 16 \t\t\t 2 \t\t\t cp29\n",
      "8 \t\t\t 20 \t\t\t 2 \t\t\t cr01\n",
      "8 \t\t\t 21 \t\t\t 2 \t\t\t cr02\n",
      "8 \t\t\t 28 \t\t\t 2 \t\t\t cr03\n",
      "8 \t\t\t 19 \t\t\t 3 \t\t\t cr04\n",
      "8 \t\t\t 17 \t\t\t 2 \t\t\t cr05\n",
      "8 \t\t\t 20 \t\t\t 3 \t\t\t cr06\n",
      "8 \t\t\t 13 \t\t\t 3 \t\t\t cr07\n",
      "8 \t\t\t 33 \t\t\t 2 \t\t\t cr08\n",
      "8 \t\t\t 23 \t\t\t 2 \t\t\t cr09\n"
     ]
    }
   ],
   "source": [
    "### Practical 2A###\n",
    "#Study of various Corpus â€“ Brown, Inaugural, Reuters, udhr with various methods like filelds, raw, words, sents, categories.\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "nltk.download('brown')\n",
    "print('File ids of brown corpus\\n',brown.fileids())\n",
    "ca01 = brown.words('ca01')\n",
    "print('\\nca01 has following words:\\n',ca01)\n",
    "print('\\nca01 has', len(ca01),'words')\n",
    "print('\\n\\nCategories or file in brown corpus:\\n')\n",
    "print(brown.categories())\n",
    "print('\\n\\nStatistics for each text:\\n')\n",
    "print('AvgWordLen\\tAvgSentenceLen\\tno.ofTimesEachWordAppearsOnAvg\\t\\tFileName')\n",
    "for fileid in brown.fileids():\n",
    "    num_chars = len(brown.raw(fileid))\n",
    "    num_words = len(brown.words(fileid))\n",
    "    num_sents = len(brown.sents(fileid))\n",
    "    num_vocab = len(set([w.lower() for w in brown.words(fileid)]))\n",
    "    print(int(num_chars/num_words),'\\t\\t\\t', int(num_words/num_sents),'\\t\\t\\t', int(num_words/num_vocab),'\\t\\t\\t',fileid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dd60374",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No such file or directory: 'C:\\\\Users\\\\admin\\\\Desktop\\\\corpus'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PlaintextCorpusReader\n\u001b[0;32m      4\u001b[0m corpus_root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/admin/Desktop/corpus\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m filelist\u001b[38;5;241m=\u001b[39m\u001b[43mPlaintextCorpusReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus_root\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.*\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m File list:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(filelist\u001b[38;5;241m.\u001b[39mfileids())\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\plaintext.py:62\u001b[0m, in \u001b[0;36mPlaintextCorpusReader.__init__\u001b[1;34m(self, root, fileids, word_tokenizer, sent_tokenizer, para_block_reader, encoding)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     39\u001b[0m     root,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     45\u001b[0m ):\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03m    Construct a new plaintext corpus reader for a set of documents\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03m    located at the given root directory.  Example usage:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m        corpus into paragraph blocks.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m     \u001b[43mCorpusReader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfileids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_word_tokenizer \u001b[38;5;241m=\u001b[39m word_tokenizer\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sent_tokenizer \u001b[38;5;241m=\u001b[39m sent_tokenizer\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\api.py:80\u001b[0m, in \u001b[0;36mCorpusReader.__init__\u001b[1;34m(self, root, fileids, encoding, tagset)\u001b[0m\n\u001b[0;32m     78\u001b[0m         root \u001b[38;5;241m=\u001b[39m ZipFilePathPointer(zipfile, zipentry)\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 80\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mFileSystemPathPointer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(root, PathPointer):\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorpusReader: expected a string or a PathPointer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\compat.py:41\u001b[0m, in \u001b[0;36mpy3_data.<locals>._decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decorator\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     40\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args[\u001b[38;5;241m0\u001b[39m], add_py3_data(args[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m+\u001b[39m args[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m init_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py:312\u001b[0m, in \u001b[0;36mFileSystemPathPointer.__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    310\u001b[0m _path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(_path)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(_path):\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such file or directory: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m _path)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m _path\n",
      "\u001b[1;31mOSError\u001b[0m: No such file or directory: 'C:\\\\Users\\\\admin\\\\Desktop\\\\corpus'"
     ]
    }
   ],
   "source": [
    "### Practical 2B\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root=\"C:/Users/admin/Desktop/corpus\"\n",
    "filelist=PlaintextCorpusReader(corpus_root,'.*')\n",
    "print('\\n File list:\\n')\n",
    "print(filelist.fileids())\n",
    "print(filelist.root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49cae224",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No such file or directory: 'C:\\\\Users\\\\admin\\\\Desktop\\\\corpus'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m filelist\u001b[38;5;241m=\u001b[39m\u001b[43mPlaintextCorpusReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus_root\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.*\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m File list:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(filelist\u001b[38;5;241m.\u001b[39mfileids())\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\plaintext.py:62\u001b[0m, in \u001b[0;36mPlaintextCorpusReader.__init__\u001b[1;34m(self, root, fileids, word_tokenizer, sent_tokenizer, para_block_reader, encoding)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     39\u001b[0m     root,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     45\u001b[0m ):\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03m    Construct a new plaintext corpus reader for a set of documents\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03m    located at the given root directory.  Example usage:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m        corpus into paragraph blocks.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m     \u001b[43mCorpusReader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfileids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_word_tokenizer \u001b[38;5;241m=\u001b[39m word_tokenizer\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sent_tokenizer \u001b[38;5;241m=\u001b[39m sent_tokenizer\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\api.py:80\u001b[0m, in \u001b[0;36mCorpusReader.__init__\u001b[1;34m(self, root, fileids, encoding, tagset)\u001b[0m\n\u001b[0;32m     78\u001b[0m         root \u001b[38;5;241m=\u001b[39m ZipFilePathPointer(zipfile, zipentry)\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 80\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mFileSystemPathPointer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(root, PathPointer):\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorpusReader: expected a string or a PathPointer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\compat.py:41\u001b[0m, in \u001b[0;36mpy3_data.<locals>._decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decorator\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     40\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args[\u001b[38;5;241m0\u001b[39m], add_py3_data(args[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m+\u001b[39m args[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m init_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py:312\u001b[0m, in \u001b[0;36mFileSystemPathPointer.__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    310\u001b[0m _path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(_path)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(_path):\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such file or directory: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m _path)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m _path\n",
      "\u001b[1;31mOSError\u001b[0m: No such file or directory: 'C:\\\\Users\\\\admin\\\\Desktop\\\\corpus'"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "print('\\n\\n Statistics for each text:\\n')\n",
    "print('AvgWordLen\\tAvgSentenceLen\\tno.ofTimesEachWordAppearsOnAvg\\tFileName')\n",
    "for fileid in filelist.fileids():\n",
    "    num_chars=len(filelist.raw(fileid))\n",
    "    num_words=len(filelist.words(fileid))\n",
    "    num_sents=len(filelist.sents(fileid))\n",
    "    num_vocab=len(set([w.lower() for w in filelist.words(fileid)]))\n",
    "    print(int(num_chars/num_words),'\\t\\t\\t',int(num_words/num_sents),'\\t\\t\\t',int(num_words/num_vocab),'\\t\\t\\t',fileid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4ff6c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sentence tokenization\n",
      "======================\n",
      " ['Hello !', 'My name is Harshal Kadam .', \"Today I'll be learning NLTK\"]\n",
      "\n",
      "word tokenization\n",
      "==================\n",
      "\n",
      "['Hello', '!']\n",
      "['My', 'name', 'is', 'Harshal', 'Kadam', '.']\n",
      "['Today', 'I', \"'ll\", 'be', 'learning', 'NLTK']\n"
     ]
    }
   ],
   "source": [
    "#Study of tagged corpora with methods like tagged_sents, tagged_words.\n",
    "\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('words')\n",
    "para = \"Hello ! My name is Harshal Kadam . Today I'll be learning NLTK\"\n",
    "sents=tokenize.sent_tokenize(para)\n",
    "print(\"\\nsentence tokenization\\n======================\\n\",sents)\n",
    "print(\"\\nword tokenization\\n==================\\n\")\n",
    "for index in range (len(sents)):\n",
    "    words=tokenize.word_tokenize(sents[index])\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a9d5621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Nick', 'NNP'), ('likes', 'VBZ'), ('to', 'TO'), ('play', 'VB'), ('football', 'NN'), ('.', '.'), ('Nick', 'NNP'), ('does', 'VBZ'), ('not', 'RB'), ('like', 'VB'), ('to', 'TO'), ('play', 'VB'), ('cricket', 'NN'), ('.', '.')]\n",
      "['Nick', 'football', 'Nick', 'cricket']\n",
      "Word with maximum frequency : Nick\n"
     ]
    }
   ],
   "source": [
    "##Write a program to find the most frequent noun tags.  \n",
    "\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "text = nltk.word_tokenize(\"Nick likes to play football. Nick does not like to play cricket.\")\n",
    "tagged = nltk.pos_tag(text)\n",
    "print(tagged)\n",
    "addNounWords = []\n",
    "count = 0\n",
    "for words in tagged:\n",
    "    val = tagged[count][1]\n",
    "    if(val == 'NN' or val =='NNS' or val == 'NNPS' or val == 'NNP'):\n",
    "        addNounWords.append(tagged[count][0])\n",
    "    count+=1\n",
    "print(addNounWords)\n",
    "temp = defaultdict(int)\n",
    "for sub in addNounWords:\n",
    "    temp[sub] += 1\n",
    "res = max(temp, key = temp.get)\n",
    "print(\"Word with maximum frequency : \" + str(res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc468917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('computer.n.01'), Synset('calculator.n.01')]\n"
     ]
    }
   ],
   "source": [
    "## Study of Wordnet Dictionary with methods as synsets, definitions, examples, antonyms \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "#nltk.download('wordnet')\n",
    "print(wordnet.synsets(\"computer\"))\n",
    "#keep each print file on new line\n",
    "print(wordnet.synset(\"computer.n.01\").definition())\n",
    "print(\"Examples:\", wordnet.synset(\"computer.n.01\").examples())\n",
    "print(wordnet.lemma('buy.v.01.buy').antonyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a7f1308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('computer.n.01'), Synset('calculator.n.01')]\n",
      "['computer', 'computing_machine', 'computing_device', 'data_processor', 'electronic_computer', 'information_processing_system']\n",
      "Synset('computer.n.01') --> ['computer', 'computing_machine', 'computing_device', 'data_processor', 'electronic_computer', 'information_processing_system']\n",
      "Synset('calculator.n.01') --> ['calculator', 'reckoner', 'figurer', 'estimator', 'computer']\n",
      "[Lemma('computer.n.01.computer'), Lemma('computer.n.01.computing_machine'), Lemma('computer.n.01.computing_device'), Lemma('computer.n.01.data_processor'), Lemma('computer.n.01.electronic_computer'), Lemma('computer.n.01.information_processing_system')]\n",
      "Synset('computer.n.01')\n",
      "computing_device\n",
      "<bound method _WordNetObject.hyponyms of Synset('computer.n.01')>\n",
      "['analog_computer', 'analogue_computer', 'digital_computer', 'home_computer', 'node', 'client', 'guest', 'number_cruncher', 'pari-mutuel_machine', 'totalizer', 'totaliser', 'totalizator', 'totalisator', 'predictor', 'server', 'host', 'Turing_machine', 'web_site', 'website', 'internet_site', 'site']\n",
      "[Synset('vehicle.n.01')]\n"
     ]
    }
   ],
   "source": [
    "#Study lemmas, hyponyms, hypernyms\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "#nltk.download('wordnet')\n",
    "print(wordnet.synsets(\"computer\"))\n",
    "print(wordnet.synset(\"computer.n.01\").lemma_names())\n",
    "for e in wordnet.synsets(\"computer\"):\n",
    "    print(f'{e} --> {e.lemma_names()}')\n",
    "print(wordnet.synset('computer.n.01').lemmas())\n",
    "print(wordnet.lemma('computer.n.01.computing_device').synset())\n",
    "print(wordnet.lemma('computer.n.01.computing_device').name())\n",
    "syn = wordnet.synset('computer.n.01')\n",
    "print(syn.hyponyms)\n",
    "print([lemma.name() for synset in syn.hyponyms() for lemma in synset.lemmas()])\n",
    "vehicle = wordnet.synset('vehicle.n.01')\n",
    "car = wordnet.synset('car.n.01')\n",
    "print(car.lowest_common_hypernyms(vehicle))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c46dcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sentence tokenization\n",
      "=================\n",
      " ['Hello!', 'My name is Harshal Kadam.', \"Today you'll be learning NLTK.\"]\n",
      "['Hello', '!']\n",
      "\n",
      "sentence tokenization\n",
      "=================\n",
      " ['Hello!', 'My name is Harshal Kadam.', \"Today you'll be learning NLTK.\"]\n",
      "['My', 'name', 'is', 'Harshal', 'Kadam', '.']\n",
      "\n",
      "sentence tokenization\n",
      "=================\n",
      " ['Hello!', 'My name is Harshal Kadam.', \"Today you'll be learning NLTK.\"]\n",
      "['Today', 'you', \"'ll\", 'be', 'learning', 'NLTK', '.']\n",
      "\n",
      "POS Tagging\n",
      "=======\n",
      "\n",
      "\n",
      "POS Tagging\n",
      "=======\n",
      "\n",
      "\n",
      "POS Tagging\n",
      "=======\n",
      "\n",
      "\n",
      "chunking\n",
      "=======\n",
      "\n",
      "[Tree('S', [('Today', 'NN'), ('you', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('learning', 'VBG'), Tree('ORGANIZATION', [('NLTK', 'NNP')]), ('.', '.')])]\n",
      "\n",
      "chunking\n",
      "=======\n",
      "\n",
      "[Tree('S', [('Today', 'NN'), ('you', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('learning', 'VBG'), Tree('ORGANIZATION', [('NLTK', 'NNP')]), ('.', '.')]), Tree('S', [('Today', 'NN'), ('you', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('learning', 'VBG'), Tree('ORGANIZATION', [('NLTK', 'NNP')]), ('.', '.')])]\n",
      "\n",
      "chunking\n",
      "=======\n",
      "\n",
      "[Tree('S', [('Today', 'NN'), ('you', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('learning', 'VBG'), Tree('ORGANIZATION', [('NLTK', 'NNP')]), ('.', '.')]), Tree('S', [('Today', 'NN'), ('you', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('learning', 'VBG'), Tree('ORGANIZATION', [('NLTK', 'NNP')]), ('.', '.')]), Tree('S', [('Today', 'NN'), ('you', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('learning', 'VBG'), Tree('ORGANIZATION', [('NLTK', 'NNP')]), ('.', '.')])]\n"
     ]
    }
   ],
   "source": [
    "# sentence tokenization, word tokenization, Part of speech Tagging and chunking of user defined text. \n",
    "\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "#nltk.download('punkt')\n",
    "from nltk import tag\n",
    "from nltk import chunk\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "for index in range(len(sents)):\n",
    "    para = \"Hello! My name is Harshal Kadam. Today you'll be learning NLTK.\"\n",
    "    sents = tokenize.sent_tokenize(para)\n",
    "    print(\"\\nsentence tokenization\\n=================\\n\",sents)\n",
    "    words = tokenize.word_tokenize(sents[index])\n",
    "    print(words)\n",
    "tagged_words = []\n",
    "for index in range(len(sents)):\n",
    "    tagged_words.append(tag.pos_tag(words))\n",
    "    print(\"\\nPOS Tagging\\n=======\\n\")\n",
    "tree = []\n",
    "for index in range(len(sents)):\n",
    "    tree.append(chunk.ne_chunk(tagged_words[index]))\n",
    "    print(\"\\nchunking\\n=======\\n\")\n",
    "    print(tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5621e252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NNP'),\n",
       " ('Vinken', 'NNP'),\n",
       " (',', ','),\n",
       " ('61', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('old', 'JJ'),\n",
       " (',', ','),\n",
       " ('will', 'MD'),\n",
       " ('join', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('board', 'NN'),\n",
       " ('as', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('nonexecutive', 'JJ'),\n",
       " ('director', 'NN'),\n",
       " ('Nov.', 'NNP'),\n",
       " ('29', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##  Named Entity recognition with diagram using NLTK corpus â€“ treebank. \n",
    "\n",
    "import nltk\n",
    "#!pip install svgling\n",
    "#nltk.download('treebank')\n",
    "from nltk.corpus import treebank_chunk\n",
    "treebank_chunk.tagged_sents()[0]\n",
    "treebank_chunk.chunked_sents()[0]\n",
    "treebank_chunk.chunked_sents()[0].draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c709f387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Book', 'that', 'flight']\n",
      "(s (VP (VP Book) (NP (Det that) (NP flight))))\n"
     ]
    }
   ],
   "source": [
    "# Chart parsing using the string \"Book that flight\"\n",
    "\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
    "s -> VP\n",
    "VP -> VP NP\n",
    "NP -> Det NP\n",
    "Det -> 'that'\n",
    "NP -> singular Noun\n",
    "NP -> 'flight'\n",
    "VP -> 'Book'\n",
    "\"\"\")\n",
    "sentence = \"Book that flight\"\n",
    "for index in range(len(sentence)):\n",
    "    all_tokens = tokenize.word_tokenize(sentence)\n",
    "print(all_tokens)\n",
    "parser = nltk.ChartParser(grammar1)\n",
    "for tree in parser.parse(all_tokens):\n",
    "    print(tree)\n",
    "    tree.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "706b1a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'saw', 'a', 'bird', 'in', 'my', 'balcony']\n",
      "(s\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (VP (V saw) (NP (Det a) (N bird)))\n",
      "    (PP (P in) (NP (Det my) (N balcony)))))\n"
     ]
    }
   ],
   "source": [
    "## Chart parsing using the string \"I saw a bird in my balcony\". \n",
    "\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
    "s -> NP VP\n",
    "NP -> 'I'\n",
    "VP -> VP PP\n",
    "VP -> V NP\n",
    "V -> 'saw'\n",
    "NP -> Det N\n",
    "Det -> 'a'\n",
    "N -> singular Noun\n",
    "N -> 'bird'\n",
    "PP -> P NP\n",
    "P -> 'in'\n",
    "NP -> Det N\n",
    "Det -> 'my'\n",
    "N -> 'balcony'\n",
    "\"\"\")\n",
    "sentence = \"I saw a bird in my balcony\"\n",
    "for index in range(len(sentence)):\n",
    "    all_tokens = tokenize.word_tokenize(sentence)\n",
    "print(all_tokens)\n",
    "parser = nltk.ChartParser(grammar1)\n",
    "for tree in parser.parse(all_tokens):\n",
    "    print(tree)\n",
    "    tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2415aca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree(S[SEM=(SELECT, City FROM city_table, WHERE, , , Country=\"china\")], [Tree(NP[SEM=(SELECT, City FROM city_table)], [Tree(Det[SEM='SELECT'], ['What']), Tree(N[SEM='City FROM city_table'], ['cities'])]), Tree(VP[SEM=(, , Country=\"china\")], [Tree(IV[SEM=''], ['are']), Tree(AP[SEM=(, Country=\"china\")], [Tree(A[SEM=''], ['located']), Tree(PP[SEM=(, Country=\"china\")], [Tree(P[SEM=''], ['in']), Tree(NP[SEM='Country=\"china\"'], ['China'])])])])])]\n",
      "(SELECT, City FROM city_table, WHERE, , , Country=\"china\")\n",
      "SELECT City FROM city_table WHERE Country=\"china\"\n",
      "('canton',)\n",
      "('chungking',)\n",
      "('dairen',)\n",
      "('harbin',)\n",
      "('kowloon',)\n",
      "('mukden',)\n",
      "('peking',)\n",
      "('shanghai',)\n",
      "('sian',)\n",
      "('tientsin',)\n"
     ]
    }
   ],
   "source": [
    "#  Analyzing the meaning of sentence by querying a database. Find the cities of India by applying a query - \n",
    "#'What cities are located in India' and the context free grammar from the file 'sqlIndia.fcfg'. \n",
    "#Hint: sqlIndia.fcfg file should be in the same folder.\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk import load_parser\n",
    "#nltk.download('book_grammars')\n",
    "#nltk.data.show_cfg('grammars/book_grammars/sql0.fcfg')\n",
    "from nltk.parse import load_parser\n",
    "cp = load_parser('grammars/book_grammars/sql0.fcfg')\n",
    "query = 'What cities are located in China'\n",
    "trees = list(cp.parse(query.split()))\n",
    "print (trees)\n",
    "answer = trees[0].label()['SEM']\n",
    "print (answer)\n",
    "answer = [s for s in answer if s]\n",
    "q = ' '.join(answer)\n",
    "print(q)\n",
    "from nltk.sem import chat80\n",
    "#nltk.download('city_database')\n",
    "rows = chat80.sql_query('corpora/city_database/city.db', q)\n",
    "for r in rows:\n",
    "  print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dff745c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([x,y],[Angus(x), dog(y), own(x,y)])\n",
      "exists x y.(Angus(x) & dog(y) & own(x,y))\n",
      "([x,z3],[Angus(x), dog(z3), own(x,z3)])\n"
     ]
    }
   ],
   "source": [
    "# Building a Discourse Representation Theory (DRT) by parsing a string representation - Angus owns a dog. \n",
    "\n",
    "#get_ipython().system('pip install nltk')\n",
    "import nltk\n",
    "read_the_expr = nltk.sem.DrtExpression.fromstring\n",
    "drs1 = read_the_expr('([x, y], [Angus(x), dog(y), own(x, y)])')\n",
    "print(drs1)\n",
    "drs1.draw()\n",
    "print(drs1.fol())\n",
    "from nltk import load_parser\n",
    "parser = load_parser('grammars/book_grammars/drt.fcfg', logic_parser=nltk.sem.drt.DrtParser())\n",
    "trees = list(parser.parse('Angus owns a dog'.split()))\n",
    "print(trees[0].label()['SEM'].simplify())\n",
    "trees[0].draw()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "49855594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write\n",
      "writ\n",
      "writ\n",
      "write\n",
      "word :\tlemma\n",
      "rocks : rock\n",
      "corpora : corpus\n",
      "better : good\n"
     ]
    }
   ],
   "source": [
    "## Study PorterStemmer, LancasterStemmer, RegexpStemmer, SnowballStemmer and WordNetLemmatizer\n",
    "\n",
    "#Change words\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "word_stemmer = PorterStemmer()\n",
    "print(word_stemmer.stem('writing'))\n",
    "\n",
    "#LancasterStemmer\n",
    "import nltk\n",
    "from nltk.stem import LancasterStemmer\n",
    "Lanc_stemmer = LancasterStemmer()\n",
    "print(Lanc_stemmer.stem('writing'))\n",
    "\n",
    "#RegexpStemmer\n",
    "import nltk\n",
    "from nltk.stem import RegexpStemmer\n",
    "Reg_stemmer = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
    "print(Reg_stemmer.stem('writing'))\n",
    " \n",
    "#SnowballStemmer\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "english_stemmer = SnowballStemmer('english')\n",
    "print(english_stemmer.stem ('writing'))\n",
    "\n",
    "#WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(\"word :\\tlemma\") \n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
    "# a denotes adjective in \"pos\"\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2b83a93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 11 productions (start state = NP)\n",
      "    NP -> NNS [0.5]\n",
      "    NP -> JJ NNS [0.3]\n",
      "    NP -> NP CC NP [0.2]\n",
      "    NNS -> 'men' [0.1]\n",
      "    NNS -> 'women' [0.2]\n",
      "    NNS -> 'children' [0.3]\n",
      "    NNS -> NNS CC NNS [0.4]\n",
      "    JJ -> 'old' [0.4]\n",
      "    JJ -> 'young' [0.6]\n",
      "    CC -> 'and' [0.9]\n",
      "    CC -> 'or' [0.1]\n",
      "Output: \n",
      "(NP (JJ old) (NNS (NNS men) (CC and) (NNS women))) (p=0.000864)\n"
     ]
    }
   ],
   "source": [
    "# a) Parse a sentence - \"old men and women\" and draw a tree using probabilistic parser\n",
    "\n",
    "import nltk\n",
    "from nltk import PCFG\n",
    "grammar = PCFG.fromstring('''\n",
    "NP -> NNS [0.5] | JJ NNS [0.3] | NP CC NP [0.2]\n",
    "NNS -> \"men\" [0.1] | \"women\" [0.2] | \"children\" [0.3] | NNS CC NNS [0.4]\n",
    "JJ -> \"old\" [0.4] | \"young\" [0.6]\n",
    "CC -> \"and\" [0.9] | \"or\" [0.1]\n",
    "''')\n",
    "print(grammar)\n",
    "viterbi_parser= nltk.ViterbiParser(grammar)\n",
    "token = \"old men and women\".split()\n",
    "obj = viterbi_parser.parse(token)\n",
    "print(\"Output: \")\n",
    "for x in obj:\n",
    "    print(x)\n",
    "    x.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9e1911db",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n\n===========================================================================\nNLTK was unable to find the maltparser-1.7.2 file!\nUse software specific configuration parameters or set the MALT_PARSER environment variable.\n===========================================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Parse a sentence - 'I saw a bird from my window.' and draw a tree using malt parsing.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#HINT:\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#Set the environment variable -> System Variable -> New -> \u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#Variable Name:(MALT-PARSER) -> Variable Value:(C:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\maltparser-1.7.2) \u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#Variable Name:(MALT-MODEL) -> Variable Value:(C:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\ engmalt.linear-1.7.mco)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m malt\n\u001b[1;32m----> 9\u001b[0m mp \u001b[38;5;241m=\u001b[39m \u001b[43mmalt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMaltParser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaltparser-1.7.2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mengmalt.linear-1.7.mco\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m t \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39mparse_one(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI saw a bird from my window.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39msplit())\u001b[38;5;241m.\u001b[39mtree()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(t)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\parse\\malt.py:143\u001b[0m, in \u001b[0;36mMaltParser.__init__\u001b[1;34m(self, parser_dirname, model_filename, tagger, additional_java_args)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;124;03mAn interface for parsing with the Malt Parser.\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;124;03m:type additional_java_args: list\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# Find all the necessary jar files for MaltParser.\u001b[39;00m\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmalt_jars \u001b[38;5;241m=\u001b[39m \u001b[43mfind_maltparser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparser_dirname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# Initialize additional java arguments.\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_java_args \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    146\u001b[0m     additional_java_args \u001b[38;5;28;01mif\u001b[39;00m additional_java_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m    147\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\parse\\malt.py:68\u001b[0m, in \u001b[0;36mfind_maltparser\u001b[1;34m(parser_dirname)\u001b[0m\n\u001b[0;32m     66\u001b[0m     _malt_dir \u001b[38;5;241m=\u001b[39m parser_dirname\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Try to find path to maltparser directory in environment variables.\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m     _malt_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparser_dirname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMALT_PARSER\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Checks that that the found directory contains all the necessary .jar\u001b[39;00m\n\u001b[0;32m     70\u001b[0m malt_dependencies \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\internals.py:649\u001b[0m, in \u001b[0;36mfind_dir\u001b[1;34m(filename, env_vars, searchpath, file_names, url, verbose)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_dir\u001b[39m(\n\u001b[0;32m    647\u001b[0m     filename, env_vars\u001b[38;5;241m=\u001b[39m(), searchpath\u001b[38;5;241m=\u001b[39m(), file_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    648\u001b[0m ):\n\u001b[1;32m--> 649\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfind_file_iter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    651\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearchpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinding_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\internals.py:635\u001b[0m, in \u001b[0;36mfind_file_iter\u001b[1;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[0;32m    633\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  For more information on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, see:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    <\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    634\u001b[0m div \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m75\u001b[39m\n\u001b[1;32m--> 635\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdiv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdiv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the maltparser-1.7.2 file!\nUse software specific configuration parameters or set the MALT_PARSER environment variable.\n==========================================================================="
     ]
    }
   ],
   "source": [
    " \n",
    "# Parse a sentence - 'I saw a bird from my window.' and draw a tree using malt parsing.\n",
    "\n",
    "#HINT:\n",
    "#Set the environment variable -> System Variable -> New -> \n",
    "#Variable Name:(MALT-PARSER) -> Variable Value:(C:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\maltparser-1.7.2) \n",
    "#Variable Name:(MALT-MODEL) -> Variable Value:(C:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\ engmalt.linear-1.7.mco)\n",
    "\n",
    "from nltk.parse import malt\n",
    "mp = malt.MaltParser('maltparser-1.7.2','engmalt.linear-1.7.mco')\n",
    "t = mp.parse_one('I saw a bird from my window.'.split()).tree()\n",
    "print(t)\n",
    "t.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dd66f76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'cake', 'cost', 'Rs.1500\\\\kg', 'in', 'New_Delhi', '.']\n",
      "['Please', 'buy', 'me', 'one', 'of', 'them', '.']\n",
      "['Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "## Multiword Expressions in NLP for multiword â€“ \n",
    "#â€˜New Delhiâ€™ in '''Good cake cost Rs.1500\\kg in New Delhi.  Please buy me one of them.\\n\\nThanks.'''\n",
    "\n",
    "\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "s = '''Good cake cost Rs.1500\\kg in New Delhi. Please buy me one of them.\\n\\nThanks.'''\n",
    "mwe = MWETokenizer([('New','Delhi'), ('New','Bombay')], separator='_')\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "for sent in sent_tokenize(s):\n",
    "    print(mwe.tokenize(word_tokenize(sent)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1bbacb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('jamming.n.01') deliberate radiation or reflection of electromagnetic energy for the purpose of disrupting enemy use of electronic devices or systems\n",
      "Synset('jam.v.05') get stuck and immobilized\n",
      "Synset('book.n.11') a number of sheets (ticket or stamps etc.) bound together on one edge\n",
      "Synset('reserve.v.04') arrange for and reserve (something for someone else) in advance\n"
     ]
    }
   ],
   "source": [
    "#Word Sense Disambiguation for the keyword â€˜jamâ€™ in the sentences - \n",
    "#'This device is used to jam the signal' and 'I am stuck in a traffic jam'. Also, for the keyword â€˜bookâ€™ in the sentences - \n",
    "#'I love reading books on coding.' and 'The table was already booked by someone else.'\n",
    "\n",
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "#nltk.download('omw-1.4')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "a1= lesk(word_tokenize('This device is used to jam the signal'),'jam')\n",
    "print(a1,a1.definition())\n",
    "a2 = lesk(word_tokenize('I am stuck in a traffic jam'),'jam')\n",
    "print(a2,a2.definition())\n",
    "b1= lesk(word_tokenize('I love reading books on coding.'),'book')\n",
    "print(b1,b1.definition())\n",
    "b2 = lesk(word_tokenize('The table was already booked by someone else.'),'book')\n",
    "print(b2,b2.definition())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c354be5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
