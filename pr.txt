### Practical 2A###
#Study of various Corpus – Brown, Inaugural, Reuters, udhr with various methods like filelds, raw, words, sents, categories.

import nltk
from nltk.corpus import brown
nltk.download('brown')
print('File ids of brown corpus\n',brown.fileids())
ca01 = brown.words('ca01')
print('\nca01 has following words:\n',ca01)
print('\nca01 has', len(ca01),'words')
print('\n\nCategories or file in brown corpus:\n')
print(brown.categories())
print('\n\nStatistics for each text:\n')
print('AvgWordLen\tAvgSentenceLen\tno.ofTimesEachWordAppearsOnAvg\t\tFileName')
for fileid in brown.fileids():
    num_chars = len(brown.raw(fileid))
    num_words = len(brown.words(fileid))
    num_sents = len(brown.sents(fileid))
    num_vocab = len(set([w.lower() for w in brown.words(fileid)]))
    print(int(num_chars/num_words),'\t\t\t', int(num_words/num_sents),'\t\t\t', int(num_words/num_vocab),'\t\t\t',fileid)



### Practical 2B
import nltk
from nltk.corpus import PlaintextCorpusReader
corpus_root="C:/Users/admin/Desktop/corpus"
filelist=PlaintextCorpusReader(corpus_root,'.*')
print('\n File list:\n')
print(filelist.fileids())
print(filelist.root)


#Study of tagged corpora with methods like tagged_sents, tagged_words.

import nltk
from nltk import tokenize
#nltk.download('punkt')
#nltk.download('words')
para = "Hello ! My name is Harshal Kadam . Today I'll be learning NLTK"
sents=tokenize.sent_tokenize(para)
print("\nsentence tokenization\n======================\n",sents)
print("\nword tokenization\n==================\n")
for index in range (len(sents)):
    words=tokenize.word_tokenize(sents[index])
    print(words)


##Write a program to find the most frequent noun tags.  

import nltk
from collections import defaultdict
#nltk.download('punkt')
#nltk.download('averaged_perceptron_tagger')
text = nltk.word_tokenize("Nick likes to play football. Nick does not like to play cricket.")
tagged = nltk.pos_tag(text)
print(tagged)
addNounWords = []
count = 0
for words in tagged:
    val = tagged[count][1]
    if(val == 'NN' or val =='NNS' or val == 'NNPS' or val == 'NNP'):
        addNounWords.append(tagged[count][0])
    count+=1
print(addNounWords)
temp = defaultdict(int)
for sub in addNounWords:
    temp[sub] += 1
res = max(temp, key = temp.get)
print("Word with maximum frequency : " + str(res))


## Study of Wordnet Dictionary with methods as synsets, definitions, examples, antonyms 

import nltk
from nltk.corpus import wordnet
#nltk.download('wordnet')
print(wordnet.synsets("computer"))
#keep each print file on new line
print(wordnet.synset("computer.n.01").definition())
print("Examples:", wordnet.synset("computer.n.01").examples())
print(wordnet.lemma('buy.v.01.buy').antonyms())


#Study lemmas, hyponyms, hypernyms

import nltk
from nltk.corpus import wordnet
#nltk.download('wordnet')
print(wordnet.synsets("computer"))
print(wordnet.synset("computer.n.01").lemma_names())
for e in wordnet.synsets("computer"):
    print(f'{e} --> {e.lemma_names()}')
print(wordnet.synset('computer.n.01').lemmas())
print(wordnet.lemma('computer.n.01.computing_device').synset())
print(wordnet.lemma('computer.n.01.computing_device').name())
syn = wordnet.synset('computer.n.01')
print(syn.hyponyms)
print([lemma.name() for synset in syn.hyponyms() for lemma in synset.lemmas()])
vehicle = wordnet.synset('vehicle.n.01')
car = wordnet.synset('car.n.01')
print(car.lowest_common_hypernyms(vehicle))



# sentence tokenization, word tokenization, Part of speech Tagging and chunking of user defined text. 

import nltk
from nltk import tokenize
#nltk.download('punkt')
from nltk import tag
from nltk import chunk
#nltk.download('averaged_perceptron_tagger')
#nltk.download('maxent_ne_chunker')
#nltk.download('words')
for index in range(len(sents)):
    para = "Hello! My name is Harshal Kadam. Today you'll be learning NLTK."
    sents = tokenize.sent_tokenize(para)
    print("\nsentence tokenization\n=================\n",sents)
    words = tokenize.word_tokenize(sents[index])
    print(words)
tagged_words = []
for index in range(len(sents)):
    tagged_words.append(tag.pos_tag(words))
    print("\nPOS Tagging\n=======\n")
tree = []
for index in range(len(sents)):
    tree.append(chunk.ne_chunk(tagged_words[index]))
    print("\nchunking\n=======\n")
    print(tree)


##  Named Entity recognition with diagram using NLTK corpus – treebank. 

import nltk
#!pip install svgling
#nltk.download('treebank')
from nltk.corpus import treebank_chunk
treebank_chunk.tagged_sents()[0]
treebank_chunk.chunked_sents()[0]
treebank_chunk.chunked_sents()[0].draw()


# Chart parsing using the string "Book that flight"

import nltk
from nltk import tokenize
grammar1 = nltk.CFG.fromstring("""
s -> VP
VP -> VP NP
NP -> Det NP
Det -> 'that'
NP -> singular Noun
NP -> 'flight'
VP -> 'Book'
""")
sentence = "Book that flight"
for index in range(len(sentence)):
    all_tokens = tokenize.word_tokenize(sentence)
print(all_tokens)
parser = nltk.ChartParser(grammar1)
for tree in parser.parse(all_tokens):
    print(tree)
    tree.draw()



## Chart parsing using the string "I saw a bird in my balcony". 

import nltk
from nltk import tokenize
grammar1 = nltk.CFG.fromstring("""
s -> NP VP
NP -> 'I'
VP -> VP PP
VP -> V NP
V -> 'saw'
NP -> Det N
Det -> 'a'
N -> singular Noun
N -> 'bird'
PP -> P NP
P -> 'in'
NP -> Det N
Det -> 'my'
N -> 'balcony'
""")
sentence = "I saw a bird in my balcony"
for index in range(len(sentence)):
    all_tokens = tokenize.word_tokenize(sentence)
print(all_tokens)
parser = nltk.ChartParser(grammar1)
for tree in parser.parse(all_tokens):
    print(tree)
    tree.draw()


#  Analyzing the meaning of sentence by querying a database. Find the cities of India by applying a query - 
#'What cities are located in India' and the context free grammar from the file 'sqlIndia.fcfg'. 
#Hint: sqlIndia.fcfg file should be in the same folder.


import nltk
from nltk import load_parser
#nltk.download('book_grammars')
#nltk.data.show_cfg('grammars/book_grammars/sql0.fcfg')
from nltk.parse import load_parser
cp = load_parser('grammars/book_grammars/sql0.fcfg')
query = 'What cities are located in China'
trees = list(cp.parse(query.split()))
print (trees)
answer = trees[0].label()['SEM']
print (answer)
answer = [s for s in answer if s]
q = ' '.join(answer)
print(q)
from nltk.sem import chat80
#nltk.download('city_database')
rows = chat80.sql_query('corpora/city_database/city.db', q)
for r in rows:
  print(r)


# Building a Discourse Representation Theory (DRT) by parsing a string representation - Angus owns a dog. 

#get_ipython().system('pip install nltk')
import nltk
read_the_expr = nltk.sem.DrtExpression.fromstring
drs1 = read_the_expr('([x, y], [Angus(x), dog(y), own(x, y)])')
print(drs1)
drs1.draw()
print(drs1.fol())
from nltk import load_parser
parser = load_parser('grammars/book_grammars/drt.fcfg', logic_parser=nltk.sem.drt.DrtParser())
trees = list(parser.parse('Angus owns a dog'.split()))
print(trees[0].label()['SEM'].simplify())
trees[0].draw()


## Study PorterStemmer, LancasterStemmer, RegexpStemmer, SnowballStemmer and WordNetLemmatizer

#Change words

import nltk
from nltk.stem import PorterStemmer
word_stemmer = PorterStemmer()
print(word_stemmer.stem('writing'))

#LancasterStemmer
import nltk
from nltk.stem import LancasterStemmer
Lanc_stemmer = LancasterStemmer()
print(Lanc_stemmer.stem('writing'))

#RegexpStemmer
import nltk
from nltk.stem import RegexpStemmer
Reg_stemmer = RegexpStemmer('ing$|s$|e$|able$', min=4)
print(Reg_stemmer.stem('writing'))
 
#SnowballStemmer
import nltk
from nltk.stem import SnowballStemmer
english_stemmer = SnowballStemmer('english')
print(english_stemmer.stem ('writing'))

#WordNetLemmatizer
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
print("word :\tlemma") 
print("rocks :", lemmatizer.lemmatize("rocks"))
print("corpora :", lemmatizer.lemmatize("corpora"))
# a denotes adjective in "pos"
print("better :", lemmatizer.lemmatize("better", pos ="a"))



# a) Parse a sentence - "old men and women" and draw a tree using probabilistic parser

import nltk
from nltk import PCFG
grammar = PCFG.fromstring('''
NP -> NNS [0.5] | JJ NNS [0.3] | NP CC NP [0.2]
NNS -> "men" [0.1] | "women" [0.2] | "children" [0.3] | NNS CC NNS [0.4]
JJ -> "old" [0.4] | "young" [0.6]
CC -> "and" [0.9] | "or" [0.1]
''')
print(grammar)
viterbi_parser= nltk.ViterbiParser(grammar)
token = "old men and women".split()
obj = viterbi_parser.parse(token)
print("Output: ")
for x in obj:
    print(x)
    x.draw()


 
# Parse a sentence - 'I saw a bird from my window.' and draw a tree using malt parsing.

#HINT:
#Set the environment variable -> System Variable -> New -> 
#Variable Name:(MALT-PARSER) -> Variable Value:(C:\Users\lenovo\AppData\Local\Programs\Python\Python310\maltparser-1.7.2) 
#Variable Name:(MALT-MODEL) -> Variable Value:(C:\Users\lenovo\AppData\Local\Programs\Python\Python310\ engmalt.linear-1.7.mco)

from nltk.parse import malt
mp = malt.MaltParser('maltparser-1.7.2','engmalt.linear-1.7.mco')
t = mp.parse_one('I saw a bird from my window.'.split()).tree()
print(t)
t.draw()


## Multiword Expressions in NLP for multiword – 
#‘New Delhi’ in '''Good cake cost Rs.1500\kg in New #Word Sense Disambiguation for the keyword ‘jam’ in the sentences - 
#'This device is used to jam the signal' and 'I am stuck in a traffic jam'. Also, for the keyword ‘book’ in the sentences - 
#'I love reading books on coding.' and 'The table was already booked by someone else.'

from nltk.wsd import lesk
from nltk.tokenize import word_tokenize
import nltk
#nltk.download('omw-1.4')
#nltk.download('punkt')
#nltk.download('wordnet')
a1= lesk(word_tokenize('This device is used to jam the signal'),'jam')
print(a1,a1.definition())
a2 = lesk(word_tokenize('I am stuck in a traffic jam'),'jam')
print(a2,a2.definition())
b1= lesk(word_tokenize('I love reading books on coding.'),'book')
print(b1,b1.definition())
b2 = lesk(word_tokenize('The table was already booked by someone else.'),'book')
print(b2,b2.definition())
Delhi.  Please buy me one of them.\n\nThanks.'''


from nltk.tokenize import MWETokenizer
from nltk import sent_tokenize, word_tokenize
s = '''Good cake cost Rs.1500\kg in New Delhi. Please buy me one of them.\n\nThanks.'''
mwe = MWETokenizer([('New','Delhi'), ('New','Bombay')], separator='_')
import nltk
#nltk.download('punkt')
for sent in sent_tokenize(s):
    print(mwe.tokenize(word_tokenize(sent)))


